# My Journey on Kaggle

Learn from amazing Kagglers on Kaggle like Radek on [twitter](https://twitter.com/radekosmulski), [kaggle](https://www.kaggle.com/radek1/code)  @cdeotte on [kaggle](https://www.kaggle.com/cdeotte/code), and a lot more I hope.
 
<mark style="background: #FFB8EBA6;">2023 Todos</mark> 
- follow @radek1 on Kaggle Playground Series, starting with Season 3, hopefully more
	- make otto comp small and iterate it fast like Playground Series
	- Kaggle competitions to explore
		- otto comp:  OTTO dataset [repo](https://github.com/otto-de/recsys-dataset#dataset-statistics) , [LeaderBoard Ranking](https://www.kaggle.com/competitions/otto-recommender-system/leaderboard#)  [discussions](https://www.kaggle.com/competitions/otto-recommender-system/discussion?sort=votes)  [notebooks](https://www.kaggle.com/competitions/otto-recommender-system/code?competitionId=38760&sortBy=voteCount) my [notebooks](https://www.kaggle.com/danielliao/code?scroll=true),
		- to replicate Radek's [journey](https://www.kaggle.com/code/radek1/fast-ai-starter-pack-train-inference) on RSNA competition 
		- to replicate DienhoaT's [journey](https://twitter.com/DienhoaT/status/1518805343425503232) on PogChamp Series competition
- diving into Zach Mueller's  [course](https://walkwithfastai.com/revisited/pets.html) and [discord](https://discord.com/channels/1033175368519131146/1041088088136495174),  [repo](https://github.com/muellerzr/Walk-with-fastai-revisited/blob/main/01_pets.ipynb)
- diving into fastai [part 1](https://github.com/fastai/course22), and part 2

<mark style="background: #FFB8EBA6;">Interesting courses and notebooks to check out</mark> 
- Distance to Cities features & ClusteringğŸ”¥ [notebook](https://www.kaggle.com/code/phongnguyen1/distance-to-cities-features-clustering?scriptVersionId=115694922)
- Effective MLOps: Model Development by W&B [course](https://www.wandb.courses/courses/take/effective-mlops-model-development/lessons/40025747-welcome-to-the-course)


<mark style="background: #FF5582A6;">Highlights of inspirations on my journey</mark> 

> I have given up on learning ML/DL before, and I came back May 2022. Thanks to your book and fastai, I won't give it up anymore. Why? 1) the super amazing fastai alumni share selflessly online, 2) I need no permission to "hold their hands" and learn from them without limit publicly. from this [tweet](https://twitter.com/shendusuipian/status/1611871815856717824?s=20&t=gYNw-VpBaKpPUNQRzYdRHw)


<mark style="background: #FFB8EBA6;">My story, my path</mark> 

- I spent 2 months to do detailed notes of courses, but notes alone does not make me a DL practitioner ğŸ˜± ğŸ˜­
- I spent 2-3 months to write a little tool to help me debug faster and help me get rid of the fear of reading and experimenting fastcore and fastai source codes ğŸ‰ ğŸ˜‚
- however, it still does not help me move toward a DL practitioner ğŸ˜± ğŸ˜­
- Thanks to Radek's book Meta-learning ğŸ’•
	- I learn to surrender my own assumptions about learning and start to embrace fastai way of learning - practice and experiment ğŸ”¥
	- I get started to learn on Kaggle and use Radek's posts and notebooks as my guide on OTTO recsys competition
- After spending more than one month in the comp, I am totally stuck on implementing a model pipeline provided by Chris and I think the main cause is I don't know pandas enough. â›” 
- ğŸ˜‚ ğŸ’• I can't say "Thank You" enough to the amazing @wasimlorgat who patiently and thoughtfully helps me understands what I want and how I learn and why I get stuck and how to unstuck, below are just a few he taught me:  ğŸ˜‚ ğŸ’• ğŸš€
	- to avoid rabbit holes like diving deep into learning pandas or polars ğŸ¦®
	- writing down my goals or todos before action to stay focused and to finish fast ğŸš€
	- not to give in to discouraging thoughts, but to recognize and celebrate every little progress along the way ğŸ’—
- In OTTO comp ğŸ”¥ [[OTTO Recsys Comp (New)]]
	- I learnt to read and follow discussions to learn small tricks to improve my public scores
	- I learnt to running and tweaking notebooks by Radek and Chris Deotte, and my previous experience in debugging help me to understand every line of their code without fear ğŸ‰ â­
	- I learnt to not give up when stuck but find easier tasks to build up my skills and come back later
- In Playground Series, Season 3 comp ğŸ”¥
	- I , for the first time, experienced and understood what Jeremy and Radek meant by 'iterate fast'
	- I changed my views on toy dataset and comps, and realized they can be powerful tools for learning new and important techniques fast
	- I have finished the goals I set when I joined the comp
- Walk with fastai course 
	- My goal is to dive deep into this course guided by Radek's [[00fastainbs/my_journey_on_kaggle/Learn from Radek#^1a922c|constraint]] principles
- 2023.1.10 Radek's newsletter helps me to identify and tack my bottleneck ğŸ”¥ğŸ”¥ğŸ”¥
	- - What's the one thing, the bottleneck for me? 
		- <mark style="background: #FF5582A6;">not build pipelines enough, not iterate pipelines enough</mark> 
	- How will I tackle this bottleneck?
		- on Kaggle, it's more like learning to build and iterate in the wild, I don't know what I will learn each day, if lucky I can find guides along the way, but in general it's more of keep exploring not knowing what is ahead
		- I learnt how to build and iterate fast from Kaggle comp like [[Playground Series Season 3, Episode 1]], and I am super excited about it because I feel this is what I am missing.
		- on fastai part1, part2, wwf, I know what's ahead is systematic and promising to build me up as a proper practitioner, but the tasks are overwhemingly massive. 
		- My plan is to <mark style="background: #BBFABBA6;">turn course notebooks and kaggle comps into building and iterating pipelines</mark> in which I will learn all the techniques of fastai in time.
- The secret to ML practitioner is Perseverance not intensity ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ [[00fastainbs/my_journey_on_kaggle/Learn from Radek#^893aa2|details]]
- keep constraint on WWF:  to dissolve WWF <mark style="background: #BBFABBA6;">course notebooks into pipeline</mark> with different set of components ğŸ˜‚ ğŸ”¥ ğŸš€  2023.1.10
- 2023.1.11, I spent a whole day learning `lineapy` hoping it can speed up iterations. However, so far I have not really got chance to use it in practice. So it is not the right time to do it and it is actually wasted me a day ğŸ˜¢ğŸ˜¢ğŸ˜¢. (It could be helpful in <mark style="background: #BBFABBA6;">reusing the code</mark> , but I have not in the place to use it yet) [[Learn from Hamel#^5b7b1c|details]]
- 2023.1.12 I wasted much time dig into `Categorize` `CropPad` which are not the central learning point of the lesson 2 of WWF. I should focus on the most important thing which is the pipeline and its components, and the detailed usage of those techniques can be learnt later when they are truly needed in practice. ğŸ˜±ğŸ˜±ğŸ˜±
- Radek's AMA is amazing, many great insights, in particular ideas on <mark style="background: #BBFABBA6;">how to subset properly, fast iteration, training on full dataset</mark> are what I needed the most at the moment. [[Learn from Radek#^73faa3|details]] ğŸ’¡âš¡ğŸ”¥ 
- After 3 days on a course, I realized that the strategy of learning which works for me is through <mark style="background: #BBFABBA6;">a ML project like kaggle comp</mark> , a course without focusing on a real-world project or competition can't keep my attention for long ğŸ˜…  ğŸ”ğŸ”ğŸ” (why? from a part of the conversation I had with Wasim)
	> Why is that? first of all, everything I learnt is what I earned by implementing and verifying the code myself, so every bit of the learning is a reward; secondly, every bit of the learning is applied in a real dataset with a real world problem, it ensures me the technique I learnt is useful in real world; thirdly, by focusing on a project/comp, a pipeline can be finished with a handful of techniques learnt, there is a nice feeling of completion (of course, there will more a lot more iterations), but with a course it could take months to absorb everything inside and still not feeling capable of doing things in the real world;
	
	