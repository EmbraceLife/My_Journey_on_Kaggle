
###  <mark style="background: #FFB8EBA6;">OTTO repo's FAQs</mark> 

How is a userÂ `session`Â defined?
-   A session is all activity by a single user either in the train or the test set.

Are there identical users in the train and test data?
-   No, **train and test users are completely disjunct**.

Are all testÂ `aids`Â included in the train set?
-   Yes, **all test items are also included in the train set**.

How can a session **start with an order or a cart**?
-   This can happen if the ordered item was already in the customer's cart before the data extraction period started. Similarly, a wishlist in our shop can lead to cart additions without a previous click.

AreÂ `aids`Â the same as article numbers onÂ [otto.de](https://github.com/otto-de/recsys-dataset/blob/main/otto.de)?
-   No, all article and session IDs are anonymized.

Are most of the clicks generated by our current recommendations?
-   No, our current recommendations generated only about 20% of the product page views in the dataset. Most users reached product pages via search results and product lists.

Are you allowed to train on the truncated test sessions?
-   Yes, for the scope of the competition, you may use all the data we provided.

is Recall@20 calculated if the ground truth contains more than 20 labels?
-   If you predict 20 items correctly out of the ground truth labels, you will still score 1.0.

Where can I find item and user metadata?
-   This dataset intentionally only contains anonymized IDs. Given its already large size, we deliberately did not include content features to make the dataset more manageable and focus on collaborative filtering techniques that solve the multi-objective problem.

---

## <mark style="background: #FFB8EBA6;">A growing reflections on otto from a beginner's perspective</mark>  [notebook](https://www.kaggle.com/code/danielliao/otto-eda-polars/) ^54ebe4

See it in my repo [page](https://github.com/EmbraceLife/My_Journey_on_Kaggle/blob/main/OTTO%20Recsys%20Comp%20(New).md#a-growing-reflections-on-otto-from-a-beginners-perspective--notebook-54ebe4) and Kaggle [post](https://www.kaggle.com/competitions/otto-recommender-system/discussion/379291) 

#### <mark style="background: #FFB86CA6;">Why?</mark> 

- writing down my reflections of learning is good for myself and others who share similar experiences
- I am a beginner who learns slowly and can't keep up with all the good and new posts and notebooks, so I will take my own pace
- otto is a worthwhile comp which I will keep learning even after the deadline
- So, this reflection will grow as I keep learning even after the deadline.
- for a quick sum for most if not all amazing posts/notebooks, please check out @thedevastator 's "One Month Left - Here is what you need to know!" [post](https://www.kaggle.com/competitions/otto-recommender-system/discussion/374229#2105455) ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥



#### <mark style="background: #FFB86CA6;">What inside the dataset</mark> 

**How the official OTTO dataset repo page describe it**

> -   12M real-world anonymized user sessions
> -   220M events, consiting ofÂ `clicks`,Â `carts`Â andÂ `orders`
> -   1.8M unique articles in the catalogue


#### <mark style="background: #FFB86CA6;">What to predict</mark> 

##### What exactly does this comp want us to predict? ğŸ’¡ğŸ’¡ğŸ’¡

thanks ğŸ™ to [@artemfedorov](https://www.kaggle.com/artemfedorov) for pointing out the misleading part of my previous description here, below is my second attempt.

##### The official OTTO dataset repo [page](https://github.com/otto-de/recsys-dataset#evaluation) describe the prediction task very precisely actually

> For eachÂ `session`Â in the test data, your task it to predict theÂ `aid`Â values for eachÂ `type`Â that occur after the last timestampÂ `ts`Â the test session. In other words, the test data contains sessions truncated by timestamp, and you are to predict what occurs after the point of truncation.

> ForÂ `clicks`Â there is only a single ground truth value for each session, which is the nextÂ `aid`Â clicked during the session (although you can still predict up to 20Â `aid`Â values). The ground truth forÂ `carts`Â andÂ `orders`Â contains allÂ `aid`Â values that were added to a cart and ordered respectively during the session.

##### Another rephrase to help understanding hopefully

^4cc0b4

- I am given a test session which has been truncated at certain timestamp
- The ground truth includes only the 1st clicked aid for each test session after the timestamp above, but I am given 20 chances to get it right
- The ground truth has all the carted aids for each session after the timestamp above, I am given 20 chances to get them all right when they are less than 20; but don't worry even when the ground truth carted aids are more than 20, I can still score full as long as I can get 20 of those carted aids right.
- The ground truth has all the ordered aids for each session after the timestamp above, I am given 20 chances to get them all right when they are less than 20; but don't worry even when the ground truth ordered aids are more than 20, I can still score full as long as I can get 20 of those ordered aids right.

The detailed rephase above is derived from the `Recall@20` metric below


#### <mark style="background: #FFB86CA6;">How to evaluate predictions</mark> 

##### How calc the `Recall@20` metric to score a type of all test sessions

$$
R_{type} = \frac{ \sum\limits_{i=1}^N | \\{ \text{predicted aids} \\}\_{i, type} \cap \\{ \text{ground truth aids} \\}\_{i, type} | }{ \sum\limits_{i=1}^N \min{( 20, | \\{ \text{ground truth aids} \\}_{i, type} | )}}
$$

Please read my interpretation of this metric in the section above on [[OTTO Recsys Comp (New)#^4cc0b4|how to interpret it]]

##### How to add up 3 different type scores above to get the total score

$$
score = 0.10 \cdot R_{clicks} + 0.30 \cdot R_{carts} + 0.60 \cdot R_{orders}
$$




#### <mark style="background: #FFB86CA6;">What insights can be derived from EDA</mark> 



##### **The official OTTO dataset repo has provided 3 tables for us**

The formula and meaning of `density` is answered by the organizer @pnormann  [here](https://github.com/otto-de/recsys-dataset/issues/2#issuecomment-1313697705) 

| dataset | num_sessions | num_items | num_events  | num_clicks  | num_carts  | num_orders | Density |
| ------- | ------------ | --------- | ----------- | ----------- | ---------- | ---------- | ------- |
| Train   | 12_899_779   | 1_855_603 | 216_716_096 | 194_720_954 | 16_896_191 | 5_098_951  | 0.0005  | 
| Test    |  1_671_803            |           |             |             |            |            |         |

|                           |  mean |   std |  min |  50% |  75% |  90% |  95% |  max |
| :------------------------ | ----: | ----: | ---: | ---: | ---: | ---: | ---: | ---: |
| Train num_events per session | 16.80 | 33.58 |    2 |    6 |   15 |   39 |   68 |  500 |
| Test num_events per session | TBA | TBA | TBA | TBA | TBA | TBA | TBA | TBA |


|                        |   mean |    std |  min |  50% |  75% |  90% |  95% |    max |
| :--------------------- | -----: | -----: | ---: | ---: | ---: | ---: | ---: | -----: |
| Train num_events per item | 116.79 | 728.85 |    3 |   20 |   56 |  183 |  398 | 129004 |
| Test num_events per item  |    TBA |    TBA |  TBA |  TBA |  TBA |  TBA |  TBA |    TBA |

##### What insights can we derive?




#### <mark style="background: #FFB86CA6;">Validation for fast iteration</mark> 


- I have created many notebooks to try to achieve this, but Radek has done it with a single notebook ğŸ˜±ğŸ˜±ğŸ˜±ğŸ˜±
- Can mine be as fast as Radek's?


#### <mark style="background: #FFB86CA6;">Candidate ReRank Model</mark> 


##### What is Candidate ReRank model? ğŸ’¡ğŸ’¡ğŸ’¡
- first we use a model to select hundreds of aid candidates, then we use another model to select the final 20 aids for predictions

##### Why Chris Deotte said Candidate ReRank model will most likely to win this comp? ğŸ’¡ğŸ’¡ğŸ’¡
- maybe it is the most obvious approach and it works in other RecSys comp, according to Chris Deotte
- there are 1.8 milliion aids to choose from, but only need 20 aids for each session_type
- It's kind of making sense to break a large problem into 2 smaller problems: choose hundreds from millions in one model, and then choose 20 from hundreds in another
- but how to select hundreds from millions? through similarities? 
	- great kagglers in otto have shared approaches like co-visitation matrix, word2vec, matrix factorization, and maybe more I don't know



####  <mark style="background: #FFB86CA6;">Co-visitation Matrix</mark> 
 

##### Radek explains it way better than my own reflection below
- "A co-visitation matrix counts the co-occurrence of two actions in close proximity."
- "If a user bought A and shortly after bought B, we store these values together."
- "We calculate counts and use them to estimate the probability of future actions based on recent history."
- "It is quite important to understand what is happening in the co-visitation matrix approachâ€¦"
- "Since it suffers from the same issues as our trigram example!"
- "Plus what does the co-visitation matrix resemble?"
- "You are right, it is akin to doing Matrix Factorization by counting!"
- "It is really fun that this competition exposed this heuristic (the co-visitation matrix) that I have not been aware of before! ğŸ™"

##### How to understand covisitation matrix intuitively? [[OTTO Recsys Comp (New)#^7f7de5|notes]] ğŸ’¡ğŸ’¡ğŸ’¡
-   itâ€™s a way to take any aid and find any number of aids which are most similar to it
-   co-visitation matrices differentiate from each other based on how they define similarity or how they select aids to be paired together

##### if you were to play the role of the inventor of co-visitation matrix, what series of ideas/questions could trigger the creation of it?
-   Is there any relationship between one aid/product with other aids/products in the same session or across all sessions?
-   Are there some aids more similar to some and more different to others?
-   Could it be possible when this aid is viewed, some aids are more likely to be clicked/carted/ordered than other aids?
-   Could we pair aids together for each and every session and count the occurrences of pairs?
-   Since one aid (eg., '122') could have many pair-partners, by counting the occurrences of the pairs ('122', pair-partner), could we find the most common pair-partners of aid '122'?
-   Could the next clicks or carts or orders be the most common pair-partners of the last aid (or all aids) of a test session?

##### what does pairing logic or a definition of simiarlity look like
-   In Radek's notebook, the pairing logic is the following
-   use only the last 30 aids of each session to pair on each other with pandas `merge` or polars `join` on `session`
-   remove the pairs of same partners
-   keep pairs whose right-partner is after left-partner within a day

##### We can tweak the pairing logic to change our co-visitation matrix

#### <mark style="background: #FFB86CA6;">Word2Vect</mark> 

##### What shortcomings does co-visitation matrix have? Could Word2Vec be a better model?
- [asked & answered](https://www.kaggle.com/competitions/otto-recommender-system/discussion/365358#2105430) Thank you @radek1 for your insightful reply again!





---

## <mark style="background: #FFB8EBA6;">My Milestone notebooks on OTTO</mark> 


- Explore otto full dataset (original in jsonl format) [notebook](https://www.kaggle.com/code/danielliao/peek-at-otto-jsonl-dataset/notebook)
- ğŸ˜± ğŸ˜‚ ğŸš€ Convert otto full dataset from jsonl to parquet and optimized in polars <mark style="background: #ABF7F7A6;">using kaggle's 30GB RAM</mark> [notebook](https://www.kaggle.com/code/danielliao/recreate-otto-full-optimized-memory-footprint)
- ğŸ˜± ğŸ˜‚ ğŸš€ Create otto validation set (jsonl, split by the last 7 days) from <mark style="background: #ABF7F7A6;">running organizer's script on Kaggle</mark> [notebook](https://www.kaggle.com/code/danielliao/otto-organizer-script-on-kaggle?scriptVersionId=114850294) [validation-by-script-on-kaggle](https://www.kaggle.com/datasets/danielliao/otto-validation-7days-jsonl-from-script-on-kaggle), ([validation-set-1](https://www.kaggle.com/datasets/danielliao/my-valid-7day), [validation-set-2](https://www.kaggle.com/datasets/danielliao/validation-7days-otto-2) created using script on paperspace) 
- Optimize and Convert otto validation set from jsonl (<mark style="background: #ABF7F7A6;">generated by organizer's script on paperspace</mark> ) to parquet in polars using kaggle's 30GB RAM [notebook-1](https://www.kaggle.com/code/danielliao/recreate-validation-7-days-parquet?scriptVersionId=114747140) ([validation-7days-parquet](https://www.kaggle.com/datasets/danielliao/ottovalidation7days)), [notebook-2](https://www.kaggle.com/code/danielliao/recreate-otto-validation-7days-2nd?scriptVersionId=114816443) ([validation-7days-2nd-parquet](https://www.kaggle.com/datasets/danielliao/ottovalidation7days2nd))
- Optimize and Convert otto validation set (except <mark style="background: #FF5582A6;">test_labels</mark> ) from jsonl (<mark style="background: #ABF7F7A6;">generated on Kaggle</mark> ) to parquet in polars on Kaggle  [notebook-3](https://www.kaggle.com/code/danielliao/otto-validation-optimized-jsonl2parquet?scriptVersionId=114887627) ([validation-optimized-parquet](https://www.kaggle.com/datasets/danielliao/otto-validation-optimized-parquet))
- ğŸ˜± ğŸ˜‚ ğŸš€ Optimize and convert otto validation set (<mark style="background: #ABF7F7A6;">full, including test_labels</mark> ) from jsonl to parquet on Kaggle with polars  [experiment](https://www.kaggle.com/code/danielliao/peek-at-otto-jsonl-dataset#Let's-peek-at-test_labels.jsonl), [notebook](https://www.kaggle.com/code/danielliao/otto-validation-optimized-jsonl2parquet?scriptVersionId=114894810) for optimization and conversion, (created the [new optimized validation dataset](https://www.kaggle.com/datasets/danielliao/otto-validation-optimized-parquet) )
- ğŸš€ ğŸ˜‚ ğŸŒŸThe [Discovery](https://twitter.com/shendusuipian/status/1607645668386164736) of a corruption of a validation set created by a Grandmaster and [conversations](https://www.kaggle.com/datasets/radek1/otto-train-and-test-data-for-local-validation/discussion/374405#2077900) with them
	- finding out which validation set has no cold start problem on aid, comparing validation from @radek1 and validations from mine [notebook](https://www.kaggle.com/danielliao/no-cold-start-aid-in-validation/) 
- ğŸ˜± ğŸ˜‚ ğŸš€ Create your own validation dataset in any length any way you want,  [notebook](https://www.kaggle.com/code/danielliao/subset-first-7days-train-test-split/notebook)
- ğŸ’¡ğŸ’¡ğŸ’¡ğŸ’¡ I need a template for subsetting the dataset in order to testing codes extremely fast no matter how small or big is the dataset, [[OTTO Recsys Comp (New)#^edf481|polars codes]], for GPU speedup, I may need a cudf code version too <mark style="background: #BBFABBA6;">todo</mark> 
- ğŸ˜± ğŸ˜‚ ğŸš€ Evaluate your submission trained on validation dataset
	- Evaluate your submission with organizer's script on kaggle,  [notebook](https://www.kaggle.com/danielliao/run-organizer-evaluation-script-directly-on-kaggle/)
	- Evaluate your submission with organizer's script implemented in polars,  [notebook](https://www.kaggle.com/danielliao/implement-organizer-evaluate-script-polars) 
- Baseline 1: the last 20 aid, implemented Radek  [notebook](https://www.kaggle.com/code/radek1/last-20-aids)  in polars, see my  [notebook](https://www.kaggle.com/code/danielliao/implement-radek-last20aids-in-polars) 
- Baseline 2:  [co-visitation matrix - simplified, imprvd logic ğŸ”¥](https://www.kaggle.com/code/radek1/co-visitation-matrix-simplified-imprvd-logic) by Radek, I implemented it in polars with my reflection on co-visitation matrix and detailed comments for codes,  [notebook](https://www.kaggle.com/code/danielliao/implement-radek-simple-covisitation-matrix-polars/)  

see all my milestone notebook at repo [page](https://github.com/EmbraceLife/My_Journey_on_Kaggle/blob/main/OTTO%20Recsys%20Comp%20(New).md#my-milestone-notebooks-on-otto)

---



## <mark style="background: #FFB8EBA6;">Following Radek into OTTO</mark>  ^4a8749

#### ğŸ˜±  Radek's [a-robust-local-validation-framework](https://www.kaggle.com/code/radek1/a-robust-local-validation-framework)  does subset, modeling, and evaluate in one go <mark style="background: #BBFABBA6;">todo, may not need it</mark>  
- let me reimplement it in polars, but it is not as fast as cudf
- let's implement this with cudf

#### check out my new dataset [subset1](https://www.kaggle.com/datasets/danielliao/1st-7days-train-validation-set) for validity, [notebook](https://www.kaggle.com/code/danielliao/verify-1week-otto-train-valid-test?scriptVersionId=116345295) <mark style="background: #ADCCFFA6;">done</mark>  2023.1.14
	- There should be no aids ofÂ `test_sessions`Â orÂ `test_labels`Â unknown toÂ `train_sessions`  <mark style="background: #ADCCFFA6;">done</mark>  
	- no overlap sessions betweenÂ `train_sessions`Â andÂ `test_sessions`  <mark style="background: #ADCCFFA6;">done</mark>  

#### How to <mark style="background: #BBFABBA6;">ensemble</mark> by Radek [notebook](https://www.kaggle.com/code/radek1/2-methods-how-to-ensemble-predictions) ğŸ”¥ğŸ”¥ğŸ”¥
ğŸš€ğŸš€ğŸš€ The template of ensemble is brilliant and ready to use. 
- âš—ï¸I can make multiple subsets and make predictions on each and ensemble them
- âš—ï¸âš—ï¸âš—ï¸ more ensembles of the same model as good as the same model on entire dataset? <mark style="background: #FF5582A6;">to test with the baseline</mark> 

ğŸ¤”ğŸ¤”ğŸ¤” but what if the predictions from different trees are more different than similar, how to pick the remaining different aids? 
- âš—ï¸âš—ï¸âš—ï¸ keep the order of list generation?  <mark style="background: #FF5582A6;">to test with the baseline</mark> 

#### Create a <mark style="background: #BBFABBA6;">baseline</mark> by Radek [notebook](https://www.kaggle.com/code/radek1/last-20-aids) with his intro [video](https://www.youtube.com/watch?v=gtPEX_eRAVo&t=400s) ğŸ”¥ğŸ”¥ğŸ”¥ 
- ğŸ’¡ğŸ’¡ğŸ’¡ *use the <mark style="background: #ABF7F7A6;">last 20 aids</mark> of each test session for predictions for clicks, carts and orders*
- ğŸ—ï¸ğŸ—ï¸ğŸ—ï¸ Let me implement it in polars [[OTTO Recsys Comp (New)#^e8e009|codes]] ,  [notebook](https://www.kaggle.com/code/danielliao/implement-radek-last20aids-in-polars)  <mark style="background: #ADCCFFA6;">done</mark>  with public [scores](https://www.kaggle.com/code/danielliao/implement-radek-last20aids-in-polars/comments#2100313) noon on 2023.1.15


#### <mark style="background: #BBFABBA6;">Co-visitation Matrix</mark> [notebook](https://www.kaggle.com/code/vslaykovsky/co-visitation-matrix) by @vslaykovsky and Radek's intro [video](https://www.youtube.com/watch?v=gtPEX_eRAVo&t=534s) ğŸ”¥ğŸ”¥ğŸ”¥ 

how co-visitation matrix is [explained](https://www.kaggle.com/code/vslaykovsky/co-visitation-matrix?scriptVersionId=110008312&cellId=1) by @vslayvkovsky ğŸš€
[co-visitation matrix - simplified, imprvd logic ğŸ”¥](https://www.kaggle.com/code/radek1/co-visitation-matrix-simplified-imprvd-logic) by Radek

ğŸ’¡ğŸ’¡ğŸ’¡ - if the last 20 aids of each test session are not enough, then append the <mark style="background: #ABF7F7A6;">most likely co-occurred aids</mark> 
ğŸ—ï¸ğŸ—ï¸ğŸ—ï¸ Let me implement it in polars, notebook started afternoon 2023.1.15, [notebook](https://www.kaggle.com/code/danielliao/implement-radek-simple-covisitation-matrix-polars/) nearly done in 2023.1.17

how similar is my implementation to Radekâ€™s ğŸ”¥ğŸ”¥ğŸ”¥
-   next_AIDs: 64 more pairs than Radekâ€™s
-   slightly faster than pandas
-   but the public score is exactly the same, 0.569 ğŸ˜‚ğŸ˜‚
-   both are super slow compared with @deotteâ€™s GPU cudf version, still need to learn pandas ğŸ˜±ğŸ˜±ğŸ˜±ğŸ’¡ğŸ’¡ğŸ’¡

key concepts and codes blocks ğŸ˜± ğŸ˜‚ âš¡ğŸ”¥
- what is co-visitation intuitively [[OTTO Recsys Comp (New)#^7f7de5|notes]] 
- how to subset and join train and test? and how to use DEBUG for fast experimenting codes? [[OTTO Recsys Comp (New)#^495ed2|codes]] 
- how to create and tweak a co-visitation matrix [[OTTO Recsys Comp (New)#^0ad2ea|codes]]
- how to create a feature (weight based on time, type and occurrences) to rerank the best 20 aids from many [[OTTO Recsys Comp (New)#^3f59e6|codes]]

how to use co-visitation matrix to select candidates from millions of aids  [[OTTO Recsys Comp (New)#^3f59e6|codes]]
-   take 20 most common aids for each aid in a test session and put them into candidate list
-   take 40 most common aids from the candidate list and add them to the aids of the test session if they are new to the session
-   then select the first 20 aids

techinques ğŸ”¥ğŸ”¥ğŸ”¥
- use `DEBUG=True` when writing the codes from start, otherwise running large dataset and failed due to code error is a huge waste of time ğŸ˜±ğŸ˜±ğŸ˜±ğŸ˜±ğŸ˜±ğŸ˜±
- I have spent most of the 2 days in implementing in polars, and realized that polars aren't fast in all situations 
- and `defaultdict` and `Counter` are fast and powerful, 
- and `sorted` is useful in sorting dict by keys or values

#### Candidate <mark style="background: #BBFABBA6;">reranking using static rules</mark> [notebook](https://www.kaggle.com/code/cdeotte/candidate-rerank-model-lb-0-575) by @cdeotte and Radek's intro [video](https://www.youtube.com/watch?v=gtPEX_eRAVo&t=773s)  ğŸ”¥ğŸ”¥ğŸ”¥ 
- use 3 co-visitation matrices to select 50 to 200 candidates from 1.6 million candidates
- use hand-crafted rules to rerank the candidates as predictions

####  <mark style="background: #BBFABBA6;">Candidate Selection Model + ReRanking Model</mark> introduced by Radek [video](https://youtu.be/gtPEX_eRAVo?t=1057) ğŸ”¥ğŸ”¥ğŸ”¥ 
- candidate selection models include co-visitation matrix, and others
- instead of hand-crafted rules, it's better to use reranking models include LGBM models [notebook](https://www.kaggle.com/code/radek1/polars-proof-of-concept-lgbm-ranker)
- ğŸ¤”ğŸ¤”ğŸ¤” how could LGBM model discover hand-crafted rules above? 

#### Second-stage Ranker <mark style="background: #BBFABBA6;">LGBM Ranker</mark> [notebook](https://www.kaggle.com/code/radek1/polars-proof-of-concept-lgbm-ranker) introduced by Radek [video](https://www.youtube.com/watch?v=gtPEX_eRAVo&t=1225s) ğŸ”¥ğŸ”¥ğŸ”¥
	- ğŸ’¡ğŸ’¡ğŸ’¡ LGBM [Ranker](https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.LGBMRanker.html) + Catboost [Ranker](https://catboost.ai/en/docs/concepts/python-reference_catboostranker) (learnt from Radek in [[Playground Series Season 3, Episode 1#^e3004d| playground series 3-1]]) 
	- what does log_recency_score look like? see my sympy plot and question asked [here](https://www.kaggle.com/code/radek1/polars-proof-of-concept-lgbm-ranker/comments#2099033), [[OTTO Recsys Comp (New)#^ec82df|codes]] 
	- how to build features and target into the dataframe for training? see Radek's [cells](https://www.kaggle.com/code/radek1/polars-proof-of-concept-lgbm-ranker?scriptVersionId=115190046&cellId=16) 
	- a blog [post](https://tamaracucumides.medium.com/learning-to-rank-with-lightgbm-code-example-in-python-843bd7b44574) on LGBM ranker

#### Word2vec model to generate candidates [video](https://www.youtube.com/watch?v=gtPEX_eRAVo&t=1335s) by Radek ğŸ”¥ğŸ”¥ğŸ”¥ [notebook](https://www.kaggle.com/code/radek1/word2vec-how-to-training-and-submission) 
	- what is sentence dataframe like in this comp
	- train the word2vec model to find the most similar aids of test aids

#### Matrix Factorization with Merlin Dataloaders [video](https://www.youtube.com/watch?v=gtPEX_eRAVo&t=1719s) and [notebook](https://www.kaggle.com/code/radek1/matrix-factorization-pytorch-merlin-dataloader) introduced by Radek
	- use the low level code of MF to introduce diversity into your MF model
	- ğŸ’¡ğŸ’¡ğŸ’¡ scores generated by MF and W2V on candidates can be used by Rerank model get better results âš¡

#### pipeline collections [notebook](https://www.kaggle.com/code/danielliao/kaggle-otto-pipeline-collections)
- **A basic pipeline** introduced by #otto_edward  with [my corrected version](https://www.kaggle.com/code/danielliao/otto-getting-started-eda-baseline?scriptVersionId=113569269&cellId=115) it can score 0.483 in BL and my polars implmentation on full dataset is [here](https://www.kaggle.com/code/danielliao/kaggle-otto-pipeline-collections?scriptVersionId=114464575) with 0.484 BL score


---

## <mark style="background: #FFB8EBA6;">Notebooks to Reimplement Organizer's script</mark> 
 
- ğŸ˜‚ ğŸš€ reimplement organizer's script in polars to create `train_sessions` or `train_valid` in otto validation set and verify its validity in this [notebook](https://www.kaggle.com/danielliao/reimplement-otto-train-validation-in-polars), [[OTTO Recsys Comp (New)#^b8c496|codes]] 
- ğŸ˜± ğŸ˜‚ ğŸš€ â­ reimplement organizer's script in polars to create `test_valid_full` or `test_sessions_full` and verify its validaty in this [notebook](https://www.kaggle.com/code/danielliao/reimplement-test-sessions-full-validation?scriptVersionId=115004300), [[OTTO Recsys Comp (New)#^2676d6|codes]],  [story](https://forums.fast.ai/t/a-beginners-attempt-at-otto-with-a-focus-on-polars/102803/7?u=daniel)
- ğŸ˜± ğŸ˜‚ ğŸš€ reimplement `test_sessions` and `test_labels` and verify its validaty [script](https://github.com/otto-de/recsys-dataset/blob/main/src/testset.py#L34) ,  [[OTTO Recsys Comp (New)#^f55509|codes-test_sessions_handmade]], [[OTTO Recsys Comp (New)#^eb3235|test_labels_handmade]] , [notebook](https://www.kaggle.com/code/danielliao/reimplement-test-sessions-labels-validation), [story](https://forums.fast.ai/t/a-beginners-attempt-at-otto-with-a-focus-on-polars/102803/9?u=daniel),  [story-continued-2](https://forums.fast.ai/t/a-beginners-attempt-at-otto-with-a-focus-on-polars/102803/10?u=daniel)  
- ğŸ˜± ğŸ˜‚ ğŸš€ reimplement organizer's `evaluate.py` script on kaggle: [notebook](https://www.kaggle.com/code/danielliao/implement-evaluate-script-otto)
	- run organizer's `evaluate.py` [script](https://github.com/otto-de/recsys-dataset/blob/0aa8346e0caec260ebd1cb47f556147cda5f770d/src/evaluate.py) on kaggle, using the evaluate [code](https://www.kaggle.com/danielliao/evaluate-otto-organizer-script/) in a pipeline [notebook](https://www.kaggle.com/danielliao/simple-pipeline-otto-1/) <mark style="background: #ADCCFFA6;">Done!</mark> 
	- ğŸ˜± ğŸ˜‚ ğŸš€ how to debugging to understand each line of the script above: [notebook](https://www.kaggle.com/danielliao/evaluate-otto-organizer-script) and story [[#^3ac7a9|inplace]] or [forum](https://forums.fast.ai/t/a-beginners-attempt-at-otto-with-a-focus-on-polars/102803/15?u=daniel) <mark style="background: #ADCCFFA6;">Done!</mark> 
	- ğŸ˜± ğŸ˜‚ ğŸš€ implement the script above in polars
		- implement `prepare_labels` and `prepare_predictions`, see [cell](https://www.kaggle.com/code/danielliao/implement-evaluate-script-otto?scriptVersionId=115288870&cellId=6) <mark style="background: #ADCCFFA6;">Done!</mark> 
		- implement `num_events(labels, k)`, see [cell](https://www.kaggle.com/code/danielliao/implement-evaluate-script-otto?scriptVersionId=115300398&cellId=16), confirmed by this [cell](https://www.kaggle.com/code/danielliao/evaluate-otto-organizer-script?scriptVersionId=115301417&cellId=7) <mark style="background: #ADCCFFA6;">Done!</mark> 
		- implement  `evaluate_session` and `evaluate_sessions`, `evaluated_events`, check script here [cell](https://www.kaggle.com/code/danielliao/evaluate-otto-organizer-script?scriptVersionId=115301417&cellId=9)  <mark style="background: #ADCCFFA6;">Done!</mark> 
			- implement `click_hits`, [cell](https://www.kaggle.com/code/danielliao/implement-evaluate-script-otto?scriptVersionId=115343714&cellId=22)<mark style="background: #ADCCFFA6;">Done!</mark> 
			- implement `cart_hits`, [cell](https://www.kaggle.com/code/danielliao/implement-evaluate-script-otto?scriptVersionId=115343714&cellId=25) <mark style="background: #ADCCFFA6;">Done!</mark> 
			- implement `order_hits`, [cell](https://www.kaggle.com/code/danielliao/implement-evaluate-script-otto?scriptVersionId=115355042&cellId=35) <mark style="background: #ADCCFFA6;">Done!</mark> 
			- join them together, [cell](https://www.kaggle.com/code/danielliao/implement-evaluate-script-otto?scriptVersionId=115355042&cellId=40) <mark style="background: #ADCCFFA6;">Done!</mark> 
			- to confirm my implementation result is the same to the organizer's result, [cell](https://www.kaggle.com/code/danielliao/implement-evaluate-script-otto?scriptVersionId=115377521&cellId=41) <mark style="background: #ADCCFFA6;">Done!</mark> 
		- implement `recall_by_event_type` and `weighted_recalls`, check script in [cell](https://www.kaggle.com/code/danielliao/implement-evaluate-script-otto?scriptVersionId=115378747&cellId=46) , and implemented [cell](https://www.kaggle.com/code/danielliao/implement-evaluate-script-otto?scriptVersionId=115380231&cellId=49), confirmed [cell](https://www.kaggle.com/code/danielliao/evaluate-otto-organizer-script?scriptVersionId=115301417&cellId=8) <mark style="background: #ADCCFFA6;">Done!</mark> 
- ğŸ˜± ğŸ˜‚ ğŸ‰ğŸ‰ using reimplementation notebooks above to split any subset of `train` into `train_sessions`, `test_sessions` and `test_labels` for fast experimentation on training and evaluating. [notebook](https://www.kaggle.com/code/danielliao/subset-first-7days-train-test-split/notebook) to create your own validation dataset, and  [notebook](https://www.kaggle.com/danielliao/run-organizer-evaluation-script-directly-on-kaggle/) to evaluate the submission based on validation dataset
	- integrate my implementations together for `train_sessions`, `test_sessions_full`, `test_sessions`, `test_labels` <mark style="background: #ADCCFFA6;">start late on 2023.1.12, done 2023.1.13 morning</mark>
	- make a proper subset (or more) from training set (4 weeks) for last iteration <mark style="background: #ADCCFFA6;">start early on 2023.1.13</mark> 
		- When the training set start and end and how the data is extracted according to time, [[OTTO Recsys Comp (New)#^8fbdee|codes]] ğŸ‰
		- How to select 7 days from the first day's 22:00 to the last day's 22:00, [[OTTO Recsys Comp (New)#^a94741|codes]] ğŸ‰
		- Let's get `train_sessions_full`, `train_sessions`, `test_sessions_full`, `test_sessions`, `test_labels`  out of it by spliting from the last 2 days,  [notebook](https://www.kaggle.com/code/danielliao/subset-first-7days-train-test-split/notebook) [dataset](https://www.kaggle.com/datasets/danielliao/1st-7days-train-validation-set) <mark style="background: #ADCCFFA6;">done end of 2023.1.13</mark>  ğŸ‰ [[OTTO Recsys Comp (New)#^9bc9b7|codes for verifying dataset]] 
		- also figured out how to deal with `datetime` and `duration` in polars [[OTTO Recsys Comp (New)#^54ebe4|details]] ğŸ”¥ğŸ‰ğŸ‰
	- integrate my implementations on evaluation <mark style="background: #ADCCFFA6;">done</mark> 
		- use organizer's evaluation script to evaluate your submission based on a validation dataset, see [notebook](https://www.kaggle.com/danielliao/run-organizer-evaluation-script-directly-on-kaggle/)
		- implement organizer's evaluation script in polars, see [notebook](https://www.kaggle.com/danielliao/implement-organizer-evaluate-script-polars) 
	- 


---

## <mark style="background: #FFB8EBA6;">Notebooks to Verify My Dataset</mark> 

Are my handmade `train`, `test` of full dataset, and `train_sessions`, `test_sessions_full`, `test_sessions`, `test_labels`  of validation set the same to the ones generated by organizer's script?
-  ğŸ˜‚ â­ Compare my `train.parquet` and `test.parquet`  from my [otto-radek-style-polars](https://www.kaggle.com/datasets/danielliao/otto-radek-style-polars) with Radek's `train` and `test` from [otto-full-optimized-memory-footprint](https://www.kaggle.com/datasets/radek1/otto-full-optimized-memory-footprint): <mark style="background: #ADCCFFA6;">Done</mark> ! experiment [notebook](https://www.kaggle.com/danielliao/compare-train-test-full-with-radek) (proved the same)
- ğŸ˜‚ â­ Compare my `train_ms.parquet` and `test_ms.parquet` with those from Colum2131's [otto-chunk-data-inparquet-format ](https://www.kaggle.com/datasets/columbia2131/otto-chunk-data-inparquet-format) (need [processing](https://www.kaggle.com/code/cdeotte/compute-validation-score-cv-565?scriptVersionId=111214251&cellId=5)): <mark style="background: #ADCCFFA6;">Done!</mark> (Same)  [notebook](https://www.kaggle.com/danielliao/compare-train-test-full-ms-with-cdeotte) 
- ğŸ˜‚ â­ Compare my `train_sessions` and `test_sessions_full` with those of [validation-7days-parquet](https://www.kaggle.com/datasets/danielliao/ottovalidation7days), [validation-7days-2nd-parquet](https://www.kaggle.com/datasets/danielliao/ottovalidation7days2nd), [new optimized validation dataset](https://www.kaggle.com/datasets/danielliao/otto-validation-optimized-parquet): <mark style="background: #ADCCFFA6;">Done!</mark> (Same! but radek's train is in different length, due to his using of old script) [notebook](https://www.kaggle.com/danielliao/compare-train-test-full-validation/)
- ğŸ˜‚ â­ Compare my `test_sessions` and `test_labels` with those of 3rd [dataset](https://www.kaggle.com/datasets/danielliao/otto-validation-optimized-parquet) and 4th validation sets (jsonl [dataset](https://www.kaggle.com/datasets/danielliao/otto-validation-4th-jsonl) and [notebook](https://www.kaggle.com/code/danielliao/4th-validation-set-jsonl?scriptVersionId=115160947), optimized parquet [dataset](https://www.kaggle.com/datasets/danielliao/validation-4th-optimized-parquet) and [notebook](https://www.kaggle.com/danielliao/4th-otto-validation-optimized-jsonl2parquet)), (both 3rd and 4th validation sets are made on Kaggle): <mark style="background: #ADCCFFA6;">Done!</mark> (Same) [notebook](https://www.kaggle.com/code/danielliao/compare-test-and-labels-validation/)
- ğŸ˜‚ â­ Compare my  `test_sessions` and `test_labels` with those of 1st validation set ([notebook](https://www.kaggle.com/danielliao/1st-otto-validation-optimized-jsonl2parque/), optimized parquet [dataset](https://www.kaggle.com/datasets/danielliao/validation-optimized-parquet-1st)) and 2nd validation set ([notebook](https://www.kaggle.com/danielliao/2nd-otto-validation-optimized-jsonl2parque/) and optimized parquet [dataset](https://www.kaggle.com/datasets/danielliao/otto-validation-optimized-parquet-2nd)): <mark style="background: #ADCCFFA6;">Done!</mark> (Same) [notebook](https://www.kaggle.com/danielliao/compare-test-and-labels-validation-1st2nd)
- ğŸ˜‚ â­ Compare 5th validation set (jsonl [datast](https://www.kaggle.com/datasets/danielliao/otto-validation-jsonl5th) created on paperspace without pipenv, [notebook](https://www.kaggle.com/danielliao/5th-otto-validation-optimized-jsonl2parque/) to create optimized-parquet [dataset](https://www.kaggle.com/datasets/danielliao/otto-validation-optimized-parquet-5th) on Kaggle) with 4th validation set: <mark style="background: #ADCCFFA6;">Done!</mark> (validation 1st, 2nd, 5th are the same as their jsonls are created on paperspace, even when 5th is created without pipenv ) [notebook](https://www.kaggle.com/code/danielliao/compare-test-and-labels-valid-4vs5), [story](https://forums.fast.ai/t/a-beginners-attempt-at-otto-with-a-focus-on-polars/102803/13?u=daniel)


---


## <mark style="background: #FFB8EBA6;">Datasets Safe and Easy to Use</mark> 

- otto-train-set-test-set-optimized (both seconds and milliseconds, generated purely on Kaggle): [otto-radek-style-polars](https://www.kaggle.com/datasets/danielliao/otto-radek-style-polars)
- otto-validation-split-7-days (generated purely on Kaggle): [validation-4th-optimized-parquet](https://www.kaggle.com/datasets/danielliao/validation-4th-optimized-parquet)
- Radek's validation [dataset](https://www.kaggle.com/datasets/radek1/otto-train-and-test-data-for-local-validation)
- Radek's full [dataset](https://www.kaggle.com/datasets/radek1/otto-full-optimized-memory-footprint)  
---


## <mark style="background: #FFB8EBA6;">Basic Resources</mark> 

otto comp: OTTO datasetÂ [repo](https://github.com/otto-de/recsys-dataset#dataset-statistics)Â ,Â [LeaderBoard Ranking](https://www.kaggle.com/competitions/otto-recommender-system/leaderboard#)Â [discussions](https://www.kaggle.com/competitions/otto-recommender-system/discussion?sort=votes)Â [notebooks](https://www.kaggle.com/competitions/otto-recommender-system/code?competitionId=38760&sortBy=voteCount)Â myÂ [notebooks](https://www.kaggle.com/danielliao/code?scroll=true), my journey [repo](https://github.com/EmbraceLife/My_Journey_on_Kaggle) 

new notebooks to explore: 
- One Month Left - Here is what you need to know! [post](https://www.kaggle.com/competitions/otto-recommender-system/discussion/374229#2105455) ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥
- best [EDA](https://www.kaggle.com/code/cdeotte/time-series-eda-users-and-real-sessions) 
- a faster Candidate ReRanker [notebook](https://www.kaggle.com/code/adaubas/otto-fast-handcrafted-model), 
-  NN [model](https://www.kaggle.com/competitions/otto-recommender-system/discussion/370756#2056631) 
- a notebook using [fastai](https://www.kaggle.com/code/shravankumar147/can-we-use-fastai) 
- handcraft [improve](https://www.kaggle.com/code/tuongkhang/otto-pipeline2-lb-0-576/comments#2049094) on deotte's Co-visitation Candidate ReRank model
- [great example on how to go from basic with baselines](https://www.kaggle.com/code/junjitakeshima/otto-easy-understanding-for-beginner-en/comments#2057777), 
- build on Radek's [w2v](https://www.kaggle.com/code/alexandershumilin/otto-word2vec/comments#2048246)
- [EDA eg](https://www.kaggle.com/code/adaubas/otto-interesting-times-series-eda-on-products/notebook), 
- the best [EDA](https://www.kaggle.com/code/edwardcrookenden/otto-getting-started-eda-baseline) 






---

## <mark style="background: #FFB86CA6;">My codes</mark> 




```python
!pip install polars
!pip install snoop
from collections import defaultdict, Counter
import gc
from snoop import pp
import polars as pl
import pandas as pd
import numpy as np
import random
from polars.testing import assert_frame_equal, assert_series_equal
from datetime import datetime

from IPython.core.interactiveshell import InteractiveShell
InteractiveShell.ast_node_interactivity = "all"
pd.set_option('display.max_colwidth', None)
cfg = pl.Config.restore_defaults()  
pl.Config.set_tbl_rows(50)  
pl.Config.set_fmt_str_lengths(1000)
```

^0769ee


```python
# Radek's validation set is using the older version of organizer's script (there is a little cold start problem on aid)
train_v = pl.scan_parquet('/kaggle/input/otto-train-and-test-data-for-local-validation/train.parquet')
test_v = pl.scan_parquet('/kaggle/input/otto-train-and-test-data-for-local-validation/test.parquet')

# I created this validation set on paperspace and optimized on Kaggle
train_v7 = pl.scan_parquet('/kaggle/input/ottovalidation7days/train_sessions.parquet')
test_v7 = pl.scan_parquet('/kaggle/input/ottovalidation7days/test_sessions.parquet')
test_v7_full = pl.scan_parquet('/kaggle/input/ottovalidation7days/test_sessions_full.parquet')

# I created this validation set on paperspace and optimized on Kaggle too
train_v7_2nd = pl.scan_parquet('/kaggle/input/ottovalidation7days2nd/train_sessions.parquet')
test_v7_2nd = pl.scan_parquet('/kaggle/input/ottovalidation7days2nd/test_sessions.parquet')
test_v7_full_2nd = pl.scan_parquet('/kaggle/input/ottovalidation7days2nd/test_sessions_full.parquet')

# I created this validation set on Kaggle and optimized on Kaggle too
train_v7_3rd = pl.scan_parquet('/kaggle/input/otto-validation-optimized-parquet/train_sessions.parquet')
test_v7_3rd = pl.scan_parquet('/kaggle/input/otto-validation-optimized-parquet/test_sessions.parquet')
test_v7_full_3rd = pl.scan_parquet('/kaggle/input/otto-validation-optimized-parquet/test_sessions_full.parquet')

train_sessions_full = pl.scan_parquet('/kaggle/input/1st-7days-train-validation-set/train_sessions_full.parquet')
train_sessions = pl.scan_parquet('/kaggle/input/1st-7days-train-validation-set/train_sessions.parquet')
test_sessions_full = pl.scan_parquet('/kaggle/input/1st-7days-train-validation-set/test_sessions_full.parquet')
test_sessions = pl.scan_parquet('/kaggle/input/1st-7days-train-validation-set/test_sessions.parquet')
test_labels = pl.scan_parquet('/kaggle/input/1st-7days-train-validation-set/test_labels.parquet')

train_ms = pl.scan_parquet('/kaggle/input/otto-radek-style-polars/train_ms.parquet')
test_ms = pl.scan_parquet('/kaggle/input/otto-radek-style-polars/test_ms.parquet')
sample_sub = pl.scan_csv('/kaggle/input/otto-recommender-system/sample_submission.csv')
```

^0f6921



```python
train_ms = pl.scan_parquet('/kaggle/input/otto-radek-style-polars/train_ms.parquet')
test_ms = pl.scan_parquet('/kaggle/input/otto-radek-style-polars/test_ms.parquet')

split_ts_ms = train_ms.select([
    (pl.col('ts').max() - 7*24*60*60*1000).alias('split_ts')
]).collect().to_series().to_list()[0]

train_sessions = (
    train_ms
    .filter(~(pl.col('ts').first() > split_ts_ms).over('session'))
    .filter(pl.col('ts') < split_ts_ms)
    .filter((pl.col('aid').count()>=2).over('session'))
    .collect()
)    
```

^b8c496

```python
train_ms = pl.scan_parquet('/kaggle/input/otto-radek-style-polars/train_ms.parquet')
test_ms = pl.scan_parquet('/kaggle/input/otto-radek-style-polars/test_ms.parquet')

split_ts_ms = train_ms.select([
    (pl.col('ts').max() - 7*24*60*60*1000).alias('split_ts')
]).collect().to_series().to_list()[0]

# building `train_items` as it is in organizer's script https://github.com/otto-de/recsys-dataset/blob/main/src/testset.py#L100
unique_aids_train_valid = (
    train_ms
    .filter(~(pl.col('ts').first() > split_ts_ms).over('session'))
    .filter(pl.col('ts') < split_ts_ms)
    .filter((pl.col('aid').count()>=2).over('session'))
    .select([
        pl.col('aid').unique().alias('unique_aids')
    ]).collect().to_series().to_list()
)
len(unique_aids_train_valid) # 1825325

test_sessions_full = (
    train_ms
    .filter((pl.col('ts').first() > split_ts_ms).over('session')).collect() # stop being lazy early can avoid error and wrong answers
    .filter(pl.col('aid').is_in(unique_aids_train_valid)) # step 2-2: https://github.com/otto-de/recsys-dataset/blob/main/src/testset.py#L75
    .filter((pl.col('aid').count()>=2).over('session')) # step 3: https://github.com/otto-de/recsys-dataset/blob/main/src/testset.py#L76

)
```

^2676d6

```python
test_v7_full_3rd = pl.scan_parquet('/kaggle/input/otto-validation-optimized-parquet/test_sessions_full.parquet')
test_sessions_full = test_v7_full_3rd.collect()

test_sessions_full = (
    test_sessions_full
    .select([
        pl.all().shift(1).over('session') # remove the last event of each session, see code in step 5 of ground_truth
    ])
    .drop_nulls()
)


sess_length = (
    test_sessions_full
    .groupby('session').agg([
        pl.col('aid').count().alias('sess_length'),
    ])
    .sort('session')
    .select('sess_length')
    .to_series().to_list()
)

random.seed(42)
split_idx_sess = [random.randint(1, x) for x in sess_length] # random seed will work here

test_sessions_split = (
    test_sessions_full
    .groupby('session').agg([
        pl.col('aid').count().alias('sess_length'),
    ])
    .sort('session')
    .with_columns([
        pl.Series(name='split_idx', values=split_idx_sess)
    ])
)

test_sessions_handmade = (
    test_sessions_full
    .with_columns([
        pl.col('aid').cumcount().over('session').alias('event_idx_each_session'),
        pl.col('aid').count().over('session').alias('session_length'),
    ])
    .join(test_sessions_split, on='session')
    .filter(pl.col('event_idx_each_session') < pl.col('split_idx'))
    .select([
        'session',
        'aid',
        'ts',
        'type'
    ])
)
```

^f55509


```python
test_sessions_full = test_v7_full_3rd

test_labels_sessions = (
    test_sessions_full
    .collect()
    .with_columns([
        pl.col('aid').cumcount().over('session').alias('event_idx_each_session'),
        pl.col('aid').count().over('session').alias('session_length'),
    ])
    .join(test_sessions_split, on='session') # include the column 'split_idx' which take into account that the last event removed
    .filter(~(pl.col('event_idx_each_session') < pl.col('split_idx'))) # the last event of each session is preserved here.
)

test_labels = (
    test_labels_sessions
    .with_columns([
        pl.col('aid').filter(pl.col('type') == 0).first().over('session').alias('label_clicks'),# won't turned into a list
        pl.col('aid').filter(pl.col('type') == 1).unique().list().over('session').alias('label_carts'),
        pl.col('aid').filter(pl.col('type') == 2).unique().list().over('session').alias('label_orders'),        
    ])
    .with_columns([
        pl.concat_list(['label_clicks']),# make it a list
        pl.col('label_orders').arr.eval(pl.element().cast(pl.Utf8)).alias("str"), 
    ])
    .select([
        pl.all().exclude(['str', 'event_idx_each_session', 'session_length', 'sess_length', 'split_idx']),
        pl.col('str').arr.join(" ").alias('label_orders_str')
    ])
    .groupby('session').agg([
        pl.all().first()
    ])
    .sort('session')
)


label_clicks = test_labels.select([
    pl.col('session'),
    pl.lit('clicks').alias('type'),
    pl.col('label_clicks').alias('ground_truth'),
]).filter(pl.col('ground_truth').arr.first().is_not_null())


label_carts = test_labels.select([
    pl.col('session'),
    pl.lit('carts').alias('type'),
    pl.col('label_carts').alias('ground_truth'),
]).filter(pl.col('ground_truth').arr.lengths()>0)


label_orders = test_labels.select([
    pl.col('session'),
    pl.lit('orders').alias('type'),
    pl.col('label_orders').alias('ground_truth'),
]).filter(pl.col('ground_truth').arr.lengths()>0)


test_labels_handmade = pl.concat([label_clicks, label_carts, label_orders]).sort('session').select([
    pl.col('session').cast(pl.Int32),
    pl.col('type'),
    pl.col('ground_truth').arr.eval(pl.element().cast(pl.Int32)) # cast into Int32 for each element of a list in a column
])    
```

^eb3235

```python
from datetime import datetime

(
    train_ms
    .select([
        pl.lit(datetime(2022, 7, 31)).alias('2022-7-31'),
        pl.lit(datetime(2022, 8, 1)).alias('2022-8-1'),        
    ])
    .collect()
)
```

^5ac544

```python
from datetime import datetime

(
    train_ms
    .filter(pl.col('ts').cast(pl.Datetime(time_unit='ms')).is_between(datetime(2022, 8, 1), datetime(2022, 8, 2)))    
    .with_columns([
        pl.col('ts').cast(pl.Datetime).dt.with_time_unit('ms')
    ])
    .select([
        pl.col('ts').min().alias("min_datetime"), 
        pl.col('ts').max().alias("max_datetime"), 
        pl.col('ts').first().alias("first_datetime"),     
        pl.col('ts').last().alias("last_datetime"), 
    ])
    .collect()
)
```

^9a15dc

```python
train_ms = pl.scan_parquet('/kaggle/input/otto-radek-style-polars/train_ms.parquet')
test_ms = pl.scan_parquet('/kaggle/input/otto-radek-style-polars/test_ms.parquet')
train_sessions = pl.scan_parquet('/kaggle/input/validation-4th-optimized-parquet/train_sessions.parquet')
test_sessions_full = pl.scan_parquet('/kaggle/input/validation-4th-optimized-parquet/train_sessions.parquet')

(
    train_ms
    .select([
        pl.col('session').n_unique().alias('total_sessions'),
        pl.col('aid').n_unique().alias('total_aid'),    
        pl.col('session').count().alias('total_rows'),      
        pl.col('ts').min().cast(pl.Datetime).dt.with_time_unit('ms').alias("min_datetime"), 
        pl.col('ts').max().cast(pl.Datetime).dt.with_time_unit('ms').alias("max_datetime"), 
        pl.col('ts').first().cast(pl.Datetime).dt.with_time_unit('ms').alias("first_datetime"),     
        pl.col('ts').last().cast(pl.Datetime).dt.with_time_unit('ms').alias("last_datetime"), 
    ])
    .collect()
)

```

^3a334e

```python
train_ms = pl.scan_parquet('/kaggle/input/otto-radek-style-polars/train_ms.parquet')

(
    train_ms
    .select([
        pl.duration(microseconds=(pl.col('ts').last() - pl.col('ts').first())).alias('if_unit_us'),                
        pl.duration(milliseconds=(pl.col('ts').last() - pl.col('ts').first())).alias('if_unit_ms'),
        pl.duration(seconds=(pl.col('ts').last() - pl.col('ts').first())).alias('if_unit_sec'),        
    ])
     .collect()
 )
```

^69336f

```python
(
    train_ms
    .filter(pl.col('session') == 12017380)
    .select([
        pl.duration(milliseconds=(pl.col('ts').last() - pl.col('ts').first())).alias('sess_duration')
    ])
    .collect()
    
)


(
    train_ms
    .groupby('session').agg([
        (pl.col('ts').last() - pl.col('ts').first()).cast(pl.Duration(time_unit='us')).alias('duration_us'),
        (pl.col('ts').last() - pl.col('ts').first()).cast(pl.Duration(time_unit='ms')).alias('duration_ms'),        
        (pl.col('ts').last() - pl.col('ts').first()).cast(pl.Datetime(time_unit='ms')).dt.hour().alias('hr_from_dur'),
        (pl.col('ts').last() - pl.col('ts').first()).cast(pl.Duration(time_unit='ms')).dt.hours().alias('dur_in_hrs'),        
        ((pl.col('ts').last() - pl.col('ts').first()).cast(pl.Duration(time_unit='ms'))/(1000*60*60)).alias('dur_in_hrs_hand'),
        (pl.col('ts').last() - pl.col('ts').first()).cast(pl.Duration(time_unit='ms')).dt.days().alias('dur_in_days'),                
        ((pl.col('ts').last() - pl.col('ts').first()).cast(pl.Duration(time_unit='ms'))/(1000*60*60*24)).alias('dur_in_days_hand'),        
        
    ])
     .collect()
 )

```

^d51937

```python
train_ms = pl.scan_parquet('/kaggle/input/otto-radek-style-polars/train_ms.parquet')
(
    train_ms
    .select([
        pl.col('ts').min().cast(pl.Datetime(time_unit='ms')).alias("min_datetime"), 
        pl.col('ts').max().cast(pl.Datetime(time_unit='ms')).alias("max_datetime"), 
        pl.col('ts').first().cast(pl.Datetime(time_unit='ms')).alias("first_datetime"),     
        pl.col('ts').last().cast(pl.Datetime(time_unit='ms')).alias("last_datetime"), 
    ])
    .collect()
)
```

^00fed4


```python
train_ms = pl.scan_parquet('/kaggle/input/otto-radek-style-polars/train_ms.parquet')
(
    train_ms
    .with_columns([
        pl.col('ts').cast(pl.Datetime(time_unit='ms')).alias("ts"), 
        pl.col('ts').cast(pl.Datetime(time_unit='ms')).dt.year().alias("year"),         
        pl.col('ts').cast(pl.Datetime(time_unit='ms')).dt.month().alias("month"),         
        pl.col('ts').cast(pl.Datetime(time_unit='ms')).dt.day().alias("day"),  
        pl.col('ts').cast(pl.Datetime(time_unit='ms')).dt.ordinal_day().alias("ordinal_day"),         
        pl.col('ts').cast(pl.Datetime(time_unit='ms')).dt.hour().alias("hour"),             
        pl.col('ts').cast(pl.Datetime(time_unit='ms')).dt.minute().alias("min"),   
        pl.col('ts').cast(pl.Datetime(time_unit='ms')).dt.second().alias("sec"),           
#         (pl.col('ts') - pl.col('ts').first()).cast(pl.Duration(time_unit='ms')).dt.hours().over('session').alias("second"),         
    ])
    .fetch()
    
)
```

^7f5465

```python
duration_since_sessions = (
    train_ms
    .with_columns([
        pl.col('ts').cast(pl.Datetime(time_unit='ms')).alias("ts"),         
        (pl.col('ts') - pl.col('ts').first()).cast(pl.Duration(time_unit='ms')).alias('duration'),        
        (pl.col('ts') - pl.col('ts').first()).cast(pl.Duration(time_unit='ms')).dt.days().over('session').alias("days_since_start_each_sess"),        
        (pl.col('ts') - pl.col('ts').first()).cast(pl.Duration(time_unit='ms')).dt.hours().over('session').alias("hrs_since_start_each_sess"),
        (pl.col('ts') - pl.col('ts').first()).cast(pl.Duration(time_unit='ms')).dt.minutes().over('session').alias("mins_since_start_each_sess"),
        (pl.col('ts') - pl.col('ts').first()).cast(pl.Duration(time_unit='ms')).dt.seconds().over('session').alias("secs_since_start_each_sess"),        
    ])
    .collect()
)
duration_since_sessions
```

^4aec22

```python
(
    duration_since_sessions
    .groupby('session')
    .head()
    .sort('session')
)
```

^fa70d5

```python
# When the training set start and end and how the data is extracted according to time
(
    train_ms
    .with_columns([
        pl.col('ts').cast(pl.Datetime(time_unit='ms'))
    ])
    .select([
        pl.col('ts').first().alias('start_time_training_set'),
        pl.col('ts').last().alias('end_time_training_set'),        
        pl.col('ts').min().alias('min_time_training_set'),
        pl.col('ts').max().alias('max_time_training_set'),                
    ])
    .collect()
)
```

^8fbdee

```python
# How to select 7 days from the first day's 22:00 to the last day's 22:00
# the train_ms starts from 2022-07-31 22:00:00.025 and ends at 2022-08-28 21:59:59.984, so my first 7 days subset should follow this time constraint
# this following code can subset and verify the data

(
    train_ms
    .filter(pl.col('ts').cast(pl.Datetime(time_unit='ms')).is_between(datetime(2022,7,31,22,0,0), datetime(2022,8,7,22,0,0)))
    .with_columns([
        pl.col('ts').cast(pl.Datetime(time_unit='ms')), # save some typing of `cast...`
    ])
    .with_columns([
        pl.col('ts').first().over('session').alias('start_datetime_each_session'),
        pl.col('ts').last().over('session').alias('end_datetime_each_session'),     
    ])
    .groupby('session')
    .head(2)
    .sort('start_datetime_each_session')
    # .sort('end_datetime_each_session', reverse=True)
    .collect()
)
```

^a94741

```python
from IPython.core.interactiveshell import InteractiveShell
InteractiveShell.ast_node_interactivity = "all"

from sympy.plotting import plot 
from sympy import Symbol

sess_len = 10
action_num_reverse = Symbol('x')

linear_interpolation = 0.1 + ((1-0.1)/(sess_len-1))*(sess_len-action_num_reverse-1)
log_recency_score = 2**line1 - 1
p = plot(linear_interpolation, log_recency_score, (action_num_reverse, 9, 0), legend=True, show=False)
p[0].line_color = 'b'
p[1].line_color = 'r'
p.show()
```

^ec82df

```python
train_sessions_full = pl.scan_parquet('/kaggle/input/1st-7days-train-validation-set/train_sessions_full.parquet')
train_sessions = pl.scan_parquet('/kaggle/input/1st-7days-train-validation-set/train_sessions.parquet')
test_sessions_full = pl.scan_parquet('/kaggle/input/1st-7days-train-validation-set/test_sessions_full.parquet')
test_sessions = pl.scan_parquet('/kaggle/input/1st-7days-train-validation-set/test_sessions.parquet')
test_labels = pl.scan_parquet('/kaggle/input/1st-7days-train-validation-set/test_labels.parquet')

# There should be no aids ofÂ `test_sessions`Â unknown toÂ `train_sessions`
(
    test_sessions
    .select([
        pl.col('aid').is_in(train_sessions.select('aid').unique().collect().to_series().to_list()).alias('known_aid_to_train?')
    ])
    .select([
        (~pl.col('known_aid_to_train?')).alias('new_aid_to_train?')
    ])
    .select([
        pl.col('new_aid_to_train?').sum().alias('num_aids_unknown_to_train')
    ])
    .collect()
)

# There should be no overlap sessions betweenÂ `train_sessions`Â andÂ `test_sessions`
(
    test_sessions
    .select([
        pl.col('session').unique().is_in(train_sessions.select('session').unique().collect().to_series().to_list()).alias('known_sess_to_train?')
    ])
    .select([
        pl.col('known_sess_to_train?').sum().alias('num_sess_known_to_train')
    ])
    .collect()
)
```

^9bc9b7


```python
train_ms = pl.scan_parquet('/kaggle/input/otto-radek-style-polars/train_ms.parquet')
test_ms = pl.scan_parquet('/kaggle/input/otto-radek-style-polars/test_ms.parquet')
sample_sub = pl.read_csv('../input/otto-recommender-system/sample_submission.csv')

# grab the last 20 aids for each test session

joined = (
    test_ms
    .with_columns([
        pl.col('aid').cumcount().over('session').alias('idx_inside_each_sess'),
        pl.col('aid').count().over('session').alias('tot_rows_each_sess')
    ])
    .sort(['session', 'ts'], reverse=[False, False]) # corrected
    .groupby('session')
    .tail(20)
    .select(pl.exclude(['idx_inside_each_sess', 'tot_rows_each_sess']))
    .groupby('session')
    .agg([
        pl.col('aid'),
    ])
    .with_columns([
        pl.col('aid').arr.eval(pl.element().cast(pl.Utf8)).arr.join(' ').alias('labels')
    ])
    .select(pl.exclude('aid'))
    .with_columns([
        (pl.col('session').cast(pl.Utf8) + '_clicks').alias('sess_click'),
        (pl.col('session').cast(pl.Utf8) + '_carts').alias('sess_cart'),
        (pl.col('session').cast(pl.Utf8) + '_orders').alias('sess_order'),        
    ])
    .collect()
)





part1 = (
    joined
    .select([
        pl.col('sess_click').alias('session_type'),
        'labels',
        'session'
    ])
)

part2 = (
    joined
    .select([
        pl.col('sess_cart').alias('session_type'),
        'labels',
        'session'
    ])
)

part3 = (
    joined
    .select([
        pl.col('sess_order').alias('session_type'),
        'labels',
        'session'
    ])
)

submission = (
    pl.concat([part1, part2, part3])
    .sort('session')
    .select(pl.exclude('session'))

)

submission.write_csv('submission.csv')
```

^e8e009

```markdown
This is shared in this [Kaggle cell](https://www.kaggle.com/code/danielliao/implement-radek-simple-covisitation-matrix-polars?scriptVersionId=116611084#Creating-Co-visitation-Matrix---nextAIDs)

**a more concise definition to covisitation**

-   itâ€™s a way to take any aid and any number of aids which are most similar to it
-   co-visitation matrices differentiate from each other based on how they define similarity or how they select aids to be paired together


**co-visitation matrix is the outcome of exploring the following ideas/questions**

-   Is there any relationship between one aid/product with other aids/products in the same session or across all sessions?
-   Are there some aids more similar to some and more different to others?
-   Could it be possible when this aid is viewed, some aids are more likely to be clicked/carted/ordered than other aids?
-   Could we pair aids together for each and every session and count the occurrences of pairs?
-   Since one aid (eg., '122') could have many pair-partners, by counting the occurrences of the pairs ('122', pair-partner), could we find the most common pair-partners of aid '122'?
-   Could the next clicks or carts or orders be the most common pair-partners of the last aid (or all aids) of a test session?

**what does pairing logic or a definition of simiarlity look like** 

In Radek's notebook, the pairing logic is the following

-   use only the last 30 aids of each session to pair on each other with pandas `merge` or polars `join` on `session`
-   remove the pairs of same partners
-   keep pairs whose right-partner is after left-partner within a day

**We can tweak the pairing logic to change our co-visitation matrix**

```

^7f7de5


```python
# how to create and tweak a co-visitation matrix on its pairing logic or similarity definition
%%time 
next_AIDs = defaultdict(Counter)
chunk_size = 300000
for i in range(0, len(sessions), chunk_size):
    current_chunk = (
        subsets
        .filter(pl.col('session').is_between(sessions[i], sessions[np.min([i+chunk_size-1, len(sessions)-1])], closed='both'))
        .unique() # no duplicates
        .groupby('session').tail(30) # step 1
    )
    current_chunk = (
        current_chunk
        .join(current_chunk, on='session', suffix='_right')
        .sort(['session', 'aid', 'aid_right']) # nice view
        .filter(pl.col('aid') != pl.col('aid_right')) # step 2: no need for pairs of themselves
        .with_columns([
            ((pl.col('ts_right') - pl.col('ts'))/(24*60*60*1000)).alias('days_elapsed') # step 3: differentiate aid_right is after or before aid in days
        ])
        .filter((pl.col('days_elapsed')>=0) & (pl.col('days_elapsed') <=1)) # step 4: only pairs whose aid_rights are after aid within 24 hrs
    )

    # step 5: organize all the pairs in defaultdict + Counter which is super faster than pure polars solution
    for aid_x, aid_y in zip(current_chunk.select('aid').to_series().to_list(), current_chunk.select('aid_right').to_series().to_list()):
        next_AIDs[aid_x][aid_y] += 1

    print(f'{int(np.ceil(i/chunk_size))} out of {int(np.ceil(len(sessions)/chunk_size))} - {np.min([i+chunk_size-1, len(sessions)-1])} sessions are done')
len(next_AIDs)


del train_ms, subset_of_train, subsets
gc.collect()
```

^0ad2ea


```python
# how to subset and join train and test? and how to use DEBUG for fast experimenting codes?
if DEBUG: fraction_of_sessions_to_use = 0.00001
else: fraction_of_sessions_to_use = 1

train_ms = pl.scan_parquet('/kaggle/input/otto-radek-style-polars/train_ms.parquet')
test_ms = pl.scan_parquet('/kaggle/input/otto-radek-style-polars/test_ms.parquet')


%%time
lucky_sessions_train = (
    train_ms
    .select([
        pl.col('session').unique().sample(frac=fraction_of_sessions_to_use, seed=42)
    ])
    .collect()
    .to_series().to_list()
)

lucky_sessions_test = (
    test_ms
    .select([
        pl.col('session').unique().sample(frac=fraction_of_sessions_to_use, seed=42)
    ])
    .collect()
    .to_series().to_list()
)

subset_of_train = (
    train_ms
    .filter(pl.col('session').is_in(lucky_sessions_train))

)

subset_of_test = (
    test_ms
    .filter(pl.col('session').is_in(lucky_sessions_test))

)

subsets = pl.concat([subset_of_train, subset_of_test]).collect()
sessions = subsets.select('session').unique().to_series().to_list()
```

^495ed2

```python
%%time
lists_aids_types = (
    test_ms
    .unique() #
    .groupby('session')
    .agg([
        pl.col('aid').list().alias('test_session_AIDs'),
        pl.col('type').list().alias('test_session_types'),        
    ])
    .collect()
)

lists_aids_types.head()


%%time
labels = []
session_types = ['clicks', 'carts', 'orders']
no_data = 0
no_data_all_aids = 0
type_weight_multipliers = {0: 1, 1: 6, 2: 3}
test_session_AIDs = lists_aids_types.select('test_session_AIDs').to_series().to_list()
test_session_types = lists_aids_types.select('test_session_types').to_series().to_list()

# take each session's aids and types
for AIDs, types in zip(test_session_AIDs, test_session_types):

    # if the session has more than 20 aids
    if len(AIDs) >= 20: 
        # np.logspace: Return numbers spaced evenly on a log scale.
        # `-1` is to ensure the weights ranges between [0,1]
        # the weights is given to AIDs based on the time order or chronological order
        weights=np.logspace(start=0.1,stop=1,num=len(AIDs),base=2, endpoint=True)-1 
        
        # create a defaultdict for this session only
        # anything added into this dict will have a default value 0
        # try `aids_temp[1]` and `aids_temp`
        aids_temp=defaultdict(lambda: 0)
        
        # in each sess, an aid may occur multiples in multiple types at different time, 
        # the line below is to take all 3 factors into account to value the importance of this aid to the session
        # each unique aid and its aggregated weight are stored in a defaultdict
        for aid,w,t in zip(AIDs,weights,types): 
            aids_temp[aid]+= w * type_weight_multipliers[t]
          
        # let's 
        sorted_aids=[k for k, v in sorted(aids_temp.items(), key=lambda item: -item[1])]

        # when using the polars below to replace the line above, it is actually 2 times slower
        # aid = [key for (key, value) in aids_temp.items()]
        # adwt = [value for (key, value) in aids_temp.items()]
        # sorted_aids = (
        #     pl.DataFrame([aid, adwt], columns=['aid', 'weight'])
        #     .sort('weight', reverse=True)
        #     .select('aid').to_series().to_list()
        # )

        # take the 20 aids with the largest weights from this session as one list and append it into a new list `labels`
        labels.append(sorted_aids[:20])
    
    # when this session has less than 20 aids
    else:
        # reverse the order of AIDs (a list of aids of this session) and remove the duplicated aids
        AIDs = list(dict.fromkeys(AIDs[::-1])) # python version
        
        # If using this polars below to replace the line above, it is infinitely slower
        # AIDs = pl.Series('aid', AIDs).unique().reverse().to_list() # polars version

        # keep track of the length of new AIDs above
        AIDs_len_start = len(AIDs)
        

        candidates = []
        # take each unique aid of this session, access its the 20 most common pair-partners and their counts
        # insert the list of the 20 most common pair-partner aids into another list `candidates` (only a pure list )
        # in the end, this `candidates` list is a lot and has many duplicates too
        for AID in AIDs:
            if AID in next_AIDs: candidates += [aid for aid, count in next_AIDs[AID].most_common(20)]
                
        # take the 40 most common aids from `candidates`, and if they are already inside AIDs of this session, 
        # then insert them into AIDs (still a pure list because of `+`, and `append` can't do it)
        AIDs += [AID for AID, cnt in Counter(candidates).most_common(40) if AID not in AIDs]
        
        # but we still only take the first 20 aids from AIDs as this session's prediction and store it in `labels`
        labels.append(AIDs[:20])
        
        # if no candidates are generated, count 1 to `no_data`
        # if candidates == []: no_data += 1 # this variable is actually not used by Radek
        
        # keep an account of the num of aids in this session and all sessions which adding no candidates
        if AIDs_len_start == len(AIDs): no_data_all_aids += 1
```

^3f59e6


```python
# a template for testing codes extremely fast no matter how small or big is the dataset
train_ms = pl.scan_parquet('/kaggle/input/otto-radek-style-polars/train_ms.parquet')
test_ms = pl.scan_parquet('/kaggle/input/otto-radek-style-polars/test_ms.parquet')
sample_sub = pl.scan_csv('/kaggle/input/otto-recommender-system/sample_submission.csv')

DEBUG = False
if DEBUG: fraction_of_sessions_to_use = 0.00001
else: fraction_of_sessions_to_use = 1

%%time
lucky_sessions_train = (
    train_ms
    .select([
        pl.col('session').unique().sample(frac=fraction_of_sessions_to_use, seed=42)
    ])
    .collect()
    .to_series().to_list()
)

lucky_sessions_test = (
    test_ms
    .select([
        pl.col('session').unique().sample(frac=fraction_of_sessions_to_use, seed=42)
    ])
    .collect()
    .to_series().to_list()
)

subset_of_train = (
    train_ms
    .filter(pl.col('session').is_in(lucky_sessions_train))

)

subset_of_test = (
    test_ms
    .filter(pl.col('session').is_in(lucky_sessions_test))

)

subsets = pl.concat([subset_of_train, subset_of_test]).collect()
sessions = subsets.select('session').unique().to_series().to_list()
```

^edf481