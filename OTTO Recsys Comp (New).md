
<mark style="background: #FFB8EBA6;">Basic Resources</mark> 

otto comp: OTTO dataset [repo](https://github.com/otto-de/recsys-dataset#dataset-statistics) , [LeaderBoard Ranking](https://www.kaggle.com/competitions/otto-recommender-system/leaderboard#) [discussions](https://www.kaggle.com/competitions/otto-recommender-system/discussion?sort=votes) [notebooks](https://www.kaggle.com/competitions/otto-recommender-system/code?competitionId=38760&sortBy=voteCount) my [notebooks](https://www.kaggle.com/danielliao/code?scroll=true),


--- 

<mark style="background: #FFB8EBA6;">OTTO repo's FAQs</mark> 

How is a user `session` defined?
-   A session is all activity by a single user either in the train or the test set.

Are there identical users in the train and test data?
-   No, **train and test users are completely disjunct**.

Are all test `aids` included in the train set?
-   Yes, **all test items are also included in the train set**.

How can a session **start with an order or a cart**?
-   This can happen if the ordered item was already in the customer's cart before the data extraction period started. Similarly, a wishlist in our shop can lead to cart additions without a previous click.

Are `aids` the same as article numbers on [otto.de](https://github.com/otto-de/recsys-dataset/blob/main/otto.de)?
-   No, all article and session IDs are anonymized.

Are most of the clicks generated by our current recommendations?
-   No, our current recommendations generated only about 20% of the product page views in the dataset. Most users reached product pages via search results and product lists.

Are you allowed to train on the truncated test sessions?
-   Yes, for the scope of the competition, you may use all the data we provided.

is Recall@20 calculated if the ground truth contains more than 20 labels?
-   If you predict 20 items correctly out of the ground truth labels, you will still score 1.0.

Where can I find item and user metadata?
-   This dataset intentionally only contains anonymized IDs. Given its already large size, we deliberately did not include content features to make the dataset more manageable and focus on collaborative filtering techniques that solve the multi-objective problem.

---

<mark style="background: #FFB8EBA6;">Exploratory Data Analysis</mark>  [notebook](https://www.kaggle.com/code/danielliao/otto-eda-polars/) ^54ebe4

- What are the imports and settings for otto comp? [[OTTO Recsys Comp (New)#^0769ee|codes]]
- What are the most used dataset to be loaded? [[OTTO Recsys Comp (New)#^0f6921|codes]] 
- How to `cast` the `first`, `last`, `max`, `min` value of `pl.col('ts')` into `pl.Datetime(time_unit='ms')` ? [[OTTO Recsys Comp (New)#^00fed4|codes]] 🔥🔥🔥🔥
- How to change `pl.Datetime` default value (`time_unit` from 'us' to 'ms') with `with_time_unit('ms')` ? [[OTTO Recsys Comp (New)#^3a334e|codes]] [nb-ver-1](https://www.kaggle.com/code/danielliao/otto-eda-polars?scriptVersionId=116229932)🔥🔥🔥🔥
- How to split the output of `pl.Datetime(time_unit='ms')` into separete columns `dt.year()`, `dt.month()`, `dt.day()`, `dt.hour()`, `dt.minute()`, `dt.second()`? [[OTTO Recsys Comp (New)#^7f5465|codes]] 🔥🔥🔥🔥
- How to calc the `pl.Duration(time_unit='ms')` of each event since the beginning of each session in terms of `dt.days()`, `dt.hours()`, `dt.minutes()`, `dt.seconds()`? [[OTTO Recsys Comp (New)#^4aec22|codes]] 🔥🔥🔥🔥
- How to get the first few rows of each session with `groupby('session').head()`? `groupby` [api](https://pola-rs.github.io/polars/py-polars/html/reference/dataframe/groupby.html), [[OTTO Recsys Comp (New)#^fa70d5|codes]] 🔥🔥🔥🔥
- How to produce a `datetime(2023, 1, 10)` with `pl.lit`? [[OTTO Recsys Comp (New)#^5ac544|codes]], [nb-ver-2](https://www.kaggle.com/code/danielliao/otto-eda-polars?scriptVersionId=116240207)
- How to `cast` `pl.col('ts')` from `pl.Int32` to `pl.Datetime(time_unit='ms')` and `filter` with `is_between(datetime(2023,1,13), datetime(2023,1,14))`? [[OTTO Recsys Comp (New)#^9a15dc|codes]] 🔥🔥🔥🔥
- How to tell whether `pl.col('ts')` has milliseconds as unit not seconds nor microseconds with `pl.duration(microseconds=(pl.col('ts').last() - pl.col('ts').first()))`?  [[OTTO Recsys Comp (New)#^69336f|codes]] 🔥
- How to find out `duration` or `Duration` in 'us', 'ms', 'hr', 'day' for each session with `groupby('session')` and `(pl.col('ts').last() - pl.col('ts').first()).cast(pl.Datetime(time_unit='ms')).dt.hour()`  [[OTTO Recsys Comp (New)#^d51937|codes]] [ver-3](https://www.kaggle.com/code/danielliao/otto-eda-polars?scriptVersionId=116250048&cellId=10) 🔥🔥🔥
- When the training set start and end and how the data is extracted according to time, [[OTTO Recsys Comp (New)#^8fbdee|codes]] 
- How to select 7 days from the first day's 22:00 to the last day's 22:00, [[OTTO Recsys Comp (New)#^a94741|codes]]


---


<mark style="background: #FFB8EBA6;">Important Notebooks to Study</mark>  ^4a8749

- check out my new dataset [subset1](https://www.kaggle.com/datasets/danielliao/1st-7days-train-validation-set) for validity, [notebook](https://www.kaggle.com/code/danielliao/verify-1week-otto-train-valid-test?scriptVersionId=116345295) <mark style="background: #ADCCFFA6;">done</mark>  2023.1.14
	- There should be no aids of `test_sessions` or `test_labels` unknown to `train_sessions`  <mark style="background: #ADCCFFA6;">done</mark>  
	- no overlap sessions between `train_sessions` and `test_sessions`  <mark style="background: #ADCCFFA6;">done</mark>  
- How to <mark style="background: #BBFABBA6;">ensemble</mark> by Radek [notebook](https://www.kaggle.com/code/radek1/2-methods-how-to-ensemble-predictions) 🔥🔥🔥
	- 🚀🚀🚀 The template of ensemble is brilliant and ready to use. 
		- ⚗️I can make multiple subsets and make predictions on each and ensemble them
		- ⚗️⚗️⚗️ more ensembles of the same model as good as the same model on entire dataset? <mark style="background: #FF5582A6;">to test with the baseline</mark> 
	- 🤔🤔🤔 but what if the predictions from different trees are more different than similar, how to pick the remaining different aids? 
		- ⚗️⚗️⚗️ keep the order of list generation?  <mark style="background: #FF5582A6;">to test with the baseline</mark> 
- Create a <mark style="background: #BBFABBA6;">baseline</mark> by Radek [notebook](https://www.kaggle.com/code/radek1/last-20-aids) with his intro [video](https://www.youtube.com/watch?v=gtPEX_eRAVo&t=400s) 🔥🔥🔥 
	- 🚀🚀🚀 - *use the <mark style="background: #ABF7F7A6;">last 20 aids</mark> for predictions*
	- 🏗️🏗️🏗️ Let me implement it in polars, notebook  <mark style="background: #D2B3FFA6;">todo</mark> 
- <mark style="background: #BBFABBA6;">Co-visitation Matrix</mark> [notebook](https://www.kaggle.com/code/vslaykovsky/co-visitation-matrix) by @vslaykovsky and Radek's intro [video](https://www.youtube.com/watch?v=gtPEX_eRAVo&t=534s) 🔥🔥🔥 
	-  [co-visitation matrix - simplified, imprvd logic 🔥](https://www.kaggle.com/code/radek1/co-visitation-matrix-simplified-imprvd-logic) by Radek
	- 🚀🚀🚀 - if the last 20 aids are not 20 in total, then fill in the <mark style="background: #ABF7F7A6;">most likely co-occurred aids</mark> to the last aid of each test session
	- 🏗️🏗️🏗️ Let me implement it in polars, notebook
- Candidate <mark style="background: #BBFABBA6;">reranking using static rules</mark> [notebook](https://www.kaggle.com/code/cdeotte/candidate-rerank-model-lb-0-575) by @cdeotte and Radek's intro [video](https://www.youtube.com/watch?v=gtPEX_eRAVo&t=773s)  🔥🔥🔥 
	- use 3 co-visitation matrices to select 50 to 200 candidates from 1.6 million candidates
	- use hand-crafted rules to rerank the candidates as predictions
- <mark style="background: #BBFABBA6;">Candidate Selection Model + ReRanking Model</mark> introduced by Radek [video](https://youtu.be/gtPEX_eRAVo?t=1057) 🔥🔥🔥 
	- candidate selection models include co-visitation matrix, and others
	- instead of hand-crafted rules, it's better to use reranking models include LGBM models [notebook](https://www.kaggle.com/code/radek1/polars-proof-of-concept-lgbm-ranker)
	- 🤔🤔🤔 how could LGBM model discover hand-crafted rules above? 
- Second-stage Ranker <mark style="background: #BBFABBA6;">LGBM Ranker</mark> [notebook](https://www.kaggle.com/code/radek1/polars-proof-of-concept-lgbm-ranker) introduced by Radek [video](https://www.youtube.com/watch?v=gtPEX_eRAVo&t=1225s) 🔥🔥🔥
	- 💡💡💡 LGBM [Ranker](https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.LGBMRanker.html) + Catboost [Ranker](https://catboost.ai/en/docs/concepts/python-reference_catboostranker) (learnt from Radek in [[Playground Series Season 3, Episode 1#^e3004d| playground series 3-1]]) 
	- what does log_recency_score look like? see my sympy plot and question asked [here](https://www.kaggle.com/code/radek1/polars-proof-of-concept-lgbm-ranker/comments#2099033), [[OTTO Recsys Comp (New)#^ec82df|codes]] 
	- how to build features and target into the dataframe for training? see Radek's [cells](https://www.kaggle.com/code/radek1/polars-proof-of-concept-lgbm-ranker?scriptVersionId=115190046&cellId=16) 
	- a blog [post](https://tamaracucumides.medium.com/learning-to-rank-with-lightgbm-code-example-in-python-843bd7b44574) on LGBM ranker
- Word2vec model to generate candidates [video](https://www.youtube.com/watch?v=gtPEX_eRAVo&t=1335s) by Radek 🔥🔥🔥 [notebook](https://www.kaggle.com/code/radek1/word2vec-how-to-training-and-submission) 
	- what is sentence dataframe like in this comp
	- train the word2vec model to find the most similar aids of test aids
- Matrix Factorization with Merlin Dataloaders [video](https://www.youtube.com/watch?v=gtPEX_eRAVo&t=1719s) and [notebook](https://www.kaggle.com/code/radek1/matrix-factorization-pytorch-merlin-dataloader) introduced by Radek
	- use the low level code of MF to introduce diversity into your MF model
	- 💡💡💡 scores generated by MF and W2V on candidates can be used by Rerank model get better results ⚡
- pipeline collections [notebook](https://www.kaggle.com/code/danielliao/kaggle-otto-pipeline-collections)
- **A basic pipeline** introduced by #otto_edward  with [my corrected version](https://www.kaggle.com/code/danielliao/otto-getting-started-eda-baseline?scriptVersionId=113569269&cellId=115) it can score 0.483 in BL and my polars implmentation on full dataset is [here](https://www.kaggle.com/code/danielliao/kaggle-otto-pipeline-collections?scriptVersionId=114464575) with 0.484 BL score

---

<mark style="background: #FFB8EBA6;">Milestone OTTO NOTEBOOKS</mark> 

- Explore otto full dataset (original in jsonl format) [notebook](https://www.kaggle.com/code/danielliao/peek-at-otto-jsonl-dataset/notebook)
- 😱 😂 🚀 Convert otto full dataset from jsonl to parquet and optimized in polars <mark style="background: #ABF7F7A6;">using kaggle's 30GB RAM</mark> [notebook](https://www.kaggle.com/code/danielliao/recreate-otto-full-optimized-memory-footprint)
- 😱 😂 🚀 Create otto validation set (jsonl, split by the last 7 days) from <mark style="background: #ABF7F7A6;">running organizer's script on Kaggle</mark> [notebook](https://www.kaggle.com/code/danielliao/otto-organizer-script-on-kaggle?scriptVersionId=114850294) [validation-by-script-on-kaggle](https://www.kaggle.com/datasets/danielliao/otto-validation-7days-jsonl-from-script-on-kaggle), ([validation-set-1](https://www.kaggle.com/datasets/danielliao/my-valid-7day), [validation-set-2](https://www.kaggle.com/datasets/danielliao/validation-7days-otto-2) created using script on paperspace) 
- Optimize and Convert otto validation set from jsonl (<mark style="background: #ABF7F7A6;">generated by organizer's script on paperspace</mark> ) to parquet in polars using kaggle's 30GB RAM [notebook-1](https://www.kaggle.com/code/danielliao/recreate-validation-7-days-parquet?scriptVersionId=114747140) ([validation-7days-parquet](https://www.kaggle.com/datasets/danielliao/ottovalidation7days)), [notebook-2](https://www.kaggle.com/code/danielliao/recreate-otto-validation-7days-2nd?scriptVersionId=114816443) ([validation-7days-2nd-parquet](https://www.kaggle.com/datasets/danielliao/ottovalidation7days2nd))
- Optimize and Convert otto validation set (except <mark style="background: #FF5582A6;">test_labels</mark> ) from jsonl (<mark style="background: #ABF7F7A6;">generated on Kaggle</mark> ) to parquet in polars on Kaggle  [notebook-3](https://www.kaggle.com/code/danielliao/otto-validation-optimized-jsonl2parquet?scriptVersionId=114887627) ([validation-optimized-parquet](https://www.kaggle.com/datasets/danielliao/otto-validation-optimized-parquet))
- 😱 😂 🚀 Optimize and convert otto validation set (<mark style="background: #ABF7F7A6;">full, including test_labels</mark> ) from jsonl to parquet on Kaggle with polars  [experiment](https://www.kaggle.com/code/danielliao/peek-at-otto-jsonl-dataset#Let's-peek-at-test_labels.jsonl), [notebook](https://www.kaggle.com/code/danielliao/otto-validation-optimized-jsonl2parquet?scriptVersionId=114894810) for optimization and conversion, (created the [new optimized validation dataset](https://www.kaggle.com/datasets/danielliao/otto-validation-optimized-parquet) )
- 🚀 😂 🌟The [Discovery](https://twitter.com/shendusuipian/status/1607645668386164736) of a corruption of a validation set created by a Grandmaster and [conversations](https://www.kaggle.com/datasets/radek1/otto-train-and-test-data-for-local-validation/discussion/374405#2077900) with them
	- finding out which validation set has no cold start problem on aid, comparing validation from @radek1 and validations from mine [notebook](https://www.kaggle.com/danielliao/no-cold-start-aid-in-validation/) 

<mark style="background: #FFB8EBA6;">Notebooks to Reimplement Organizer's script</mark> 
 
- 😂 🚀 reimplement organizer's script in polars to create `train_sessions` or `train_valid` in otto validation set and verify its validity in this [notebook](https://www.kaggle.com/danielliao/reimplement-otto-train-validation-in-polars), [[OTTO Recsys Comp (New)#^b8c496|codes]] 
- 😱 😂 🚀 ⭐ reimplement organizer's script in polars to create `test_valid_full` or `test_sessions_full` and verify its validaty in this [notebook](https://www.kaggle.com/code/danielliao/reimplement-test-sessions-full-validation?scriptVersionId=115004300), [[OTTO Recsys Comp (New)#^2676d6|codes]],  [story](https://forums.fast.ai/t/a-beginners-attempt-at-otto-with-a-focus-on-polars/102803/7?u=daniel)
- 😱 😂 🚀 reimplement `test_sessions` and `test_labels` and verify its validaty [script](https://github.com/otto-de/recsys-dataset/blob/main/src/testset.py#L34) ,  [[OTTO Recsys Comp (New)#^f55509|codes-test_sessions_handmade]], [[OTTO Recsys Comp (New)#^eb3235|test_labels_handmade]] , [notebook](https://www.kaggle.com/code/danielliao/reimplement-test-sessions-labels-validation), [story](https://forums.fast.ai/t/a-beginners-attempt-at-otto-with-a-focus-on-polars/102803/9?u=daniel),  [story-continued-2](https://forums.fast.ai/t/a-beginners-attempt-at-otto-with-a-focus-on-polars/102803/10?u=daniel)  
- 😱 😂 🚀 reimplement organizer's `evaluate.py` script on kaggle: [notebook](https://www.kaggle.com/code/danielliao/implement-evaluate-script-otto)
	- run organizer's `evaluate.py` [script](https://github.com/otto-de/recsys-dataset/blob/0aa8346e0caec260ebd1cb47f556147cda5f770d/src/evaluate.py) on kaggle, using the evaluate [code](https://www.kaggle.com/danielliao/evaluate-otto-organizer-script/) in a pipeline [notebook](https://www.kaggle.com/danielliao/simple-pipeline-otto-1/) <mark style="background: #ADCCFFA6;">Done!</mark> 
	- 😱 😂 🚀 how to debugging to understand each line of the script above: [notebook](https://www.kaggle.com/danielliao/evaluate-otto-organizer-script) and story [[#^3ac7a9|inplace]] or [forum](https://forums.fast.ai/t/a-beginners-attempt-at-otto-with-a-focus-on-polars/102803/15?u=daniel) <mark style="background: #ADCCFFA6;">Done!</mark> 
	- 😱 😂 🚀 implement the script above in polars
		- implement `prepare_labels` and `prepare_predictions`, see [cell](https://www.kaggle.com/code/danielliao/implement-evaluate-script-otto?scriptVersionId=115288870&cellId=6) <mark style="background: #ADCCFFA6;">Done!</mark> 
		- implement `num_events(labels, k)`, see [cell](https://www.kaggle.com/code/danielliao/implement-evaluate-script-otto?scriptVersionId=115300398&cellId=16), confirmed by this [cell](https://www.kaggle.com/code/danielliao/evaluate-otto-organizer-script?scriptVersionId=115301417&cellId=7) <mark style="background: #ADCCFFA6;">Done!</mark> 
		- implement  `evaluate_session` and `evaluate_sessions`, `evaluated_events`, check script here [cell](https://www.kaggle.com/code/danielliao/evaluate-otto-organizer-script?scriptVersionId=115301417&cellId=9)  <mark style="background: #ADCCFFA6;">Done!</mark> 
			- implement `click_hits`, [cell](https://www.kaggle.com/code/danielliao/implement-evaluate-script-otto?scriptVersionId=115343714&cellId=22)<mark style="background: #ADCCFFA6;">Done!</mark> 
			- implement `cart_hits`, [cell](https://www.kaggle.com/code/danielliao/implement-evaluate-script-otto?scriptVersionId=115343714&cellId=25) <mark style="background: #ADCCFFA6;">Done!</mark> 
			- implement `order_hits`, [cell](https://www.kaggle.com/code/danielliao/implement-evaluate-script-otto?scriptVersionId=115355042&cellId=35) <mark style="background: #ADCCFFA6;">Done!</mark> 
			- join them together, [cell](https://www.kaggle.com/code/danielliao/implement-evaluate-script-otto?scriptVersionId=115355042&cellId=40) <mark style="background: #ADCCFFA6;">Done!</mark> 
			- to confirm my implementation result is the same to the organizer's result, [cell](https://www.kaggle.com/code/danielliao/implement-evaluate-script-otto?scriptVersionId=115377521&cellId=41) <mark style="background: #ADCCFFA6;">Done!</mark> 
		- implement `recall_by_event_type` and `weighted_recalls`, check script in [cell](https://www.kaggle.com/code/danielliao/implement-evaluate-script-otto?scriptVersionId=115378747&cellId=46) , and implemented [cell](https://www.kaggle.com/code/danielliao/implement-evaluate-script-otto?scriptVersionId=115380231&cellId=49), confirmed [cell](https://www.kaggle.com/code/danielliao/evaluate-otto-organizer-script?scriptVersionId=115301417&cellId=8) <mark style="background: #ADCCFFA6;">Done!</mark> 
- 😱 😂 🎉🎉 using reimplementation notebooks above to split any subset of `train` into `train_sessions`, `test_sessions` and `test_labels` for fast experimentation on training and evaluating
	- integrate my implementations together for `train_sessions`, `test_sessions_full`, `test_sessions`, `test_labels` <mark style="background: #ADCCFFA6;">start late on 2023.1.12, done 2023.1.13 morning</mark>
	- make a proper subset (or more) from training set (4 weeks) for last iteration <mark style="background: #ADCCFFA6;">start early on 2023.1.13</mark> 
		- When the training set start and end and how the data is extracted according to time, [[OTTO Recsys Comp (New)#^8fbdee|codes]] 🎉
		- How to select 7 days from the first day's 22:00 to the last day's 22:00, [[OTTO Recsys Comp (New)#^a94741|codes]] 🎉
		- Let's get `train_sessions_full`, `train_sessions`, `test_sessions_full`, `test_sessions`, `test_labels`  out of it by spliting from the last 2 days, [notebook](https://www.kaggle.com/code/danielliao/subset-first-7days-train-test-split/notebook) [dataset](https://www.kaggle.com/datasets/danielliao/1st-7days-train-validation-set) <mark style="background: #ADCCFFA6;">done end of 2023.1.13</mark>  🎉
		- also figured out how to deal with `datetime` and `duration` in polars [[OTTO Recsys Comp (New)#^54ebe4|details]] 🔥🎉🎉
	- integrate my implementations on evaluation   <mark style="background: #BBFABBA6;">Todo</mark> 
	- 😱  Radek's [a-robust-local-validation-framework](https://www.kaggle.com/code/radek1/a-robust-local-validation-framework)  does subset, modeling, and evaluate in one go, let me reimplement it in polars

<mark style="background: #FFB8EBA6;">Notebooks to Verify My Dataset</mark> 

Are my handmade `train`, `test` of full dataset, and `train_sessions`, `test_sessions_full`, `test_sessions`, `test_labels`  of validation set the same to the ones generated by organizer's script?
-  😂 ⭐ Compare my `train.parquet` and `test.parquet`  from my [otto-radek-style-polars](https://www.kaggle.com/datasets/danielliao/otto-radek-style-polars) with Radek's `train` and `test` from [otto-full-optimized-memory-footprint](https://www.kaggle.com/datasets/radek1/otto-full-optimized-memory-footprint): <mark style="background: #ADCCFFA6;">Done</mark> ! experiment [notebook](https://www.kaggle.com/danielliao/compare-train-test-full-with-radek) (proved the same)
- 😂 ⭐ Compare my `train_ms.parquet` and `test_ms.parquet` with those from Colum2131's [otto-chunk-data-inparquet-format ](https://www.kaggle.com/datasets/columbia2131/otto-chunk-data-inparquet-format) (need [processing](https://www.kaggle.com/code/cdeotte/compute-validation-score-cv-565?scriptVersionId=111214251&cellId=5)): <mark style="background: #ADCCFFA6;">Done!</mark> (Same)  [notebook](https://www.kaggle.com/danielliao/compare-train-test-full-ms-with-cdeotte) 
- 😂 ⭐ Compare my `train_sessions` and `test_sessions_full` with those of [validation-7days-parquet](https://www.kaggle.com/datasets/danielliao/ottovalidation7days), [validation-7days-2nd-parquet](https://www.kaggle.com/datasets/danielliao/ottovalidation7days2nd), [new optimized validation dataset](https://www.kaggle.com/datasets/danielliao/otto-validation-optimized-parquet): <mark style="background: #ADCCFFA6;">Done!</mark> (Same! but radek's train is in different length, due to his using of old script) [notebook](https://www.kaggle.com/danielliao/compare-train-test-full-validation/)
- 😂 ⭐ Compare my `test_sessions` and `test_labels` with those of 3rd [dataset](https://www.kaggle.com/datasets/danielliao/otto-validation-optimized-parquet) and 4th validation sets (jsonl [dataset](https://www.kaggle.com/datasets/danielliao/otto-validation-4th-jsonl) and [notebook](https://www.kaggle.com/code/danielliao/4th-validation-set-jsonl?scriptVersionId=115160947), optimized parquet [dataset](https://www.kaggle.com/datasets/danielliao/validation-4th-optimized-parquet) and [notebook](https://www.kaggle.com/danielliao/4th-otto-validation-optimized-jsonl2parquet)), (both 3rd and 4th validation sets are made on Kaggle): <mark style="background: #ADCCFFA6;">Done!</mark> (Same) [notebook](https://www.kaggle.com/code/danielliao/compare-test-and-labels-validation/)
- 😂 ⭐ Compare my  `test_sessions` and `test_labels` with those of 1st validation set ([notebook](https://www.kaggle.com/danielliao/1st-otto-validation-optimized-jsonl2parque/), optimized parquet [dataset](https://www.kaggle.com/datasets/danielliao/validation-optimized-parquet-1st)) and 2nd validation set ([notebook](https://www.kaggle.com/danielliao/2nd-otto-validation-optimized-jsonl2parque/) and optimized parquet [dataset](https://www.kaggle.com/datasets/danielliao/otto-validation-optimized-parquet-2nd)): <mark style="background: #ADCCFFA6;">Done!</mark> (Same) [notebook](https://www.kaggle.com/danielliao/compare-test-and-labels-validation-1st2nd)
- 😂 ⭐ Compare 5th validation set (jsonl [datast](https://www.kaggle.com/datasets/danielliao/otto-validation-jsonl5th) created on paperspace without pipenv, [notebook](https://www.kaggle.com/danielliao/5th-otto-validation-optimized-jsonl2parque/) to create optimized-parquet [dataset](https://www.kaggle.com/datasets/danielliao/otto-validation-optimized-parquet-5th) on Kaggle) with 4th validation set: <mark style="background: #ADCCFFA6;">Done!</mark> (validation 1st, 2nd, 5th are the same as their jsonls are created on paperspace, even when 5th is created without pipenv ) [notebook](https://www.kaggle.com/code/danielliao/compare-test-and-labels-valid-4vs5), [story](https://forums.fast.ai/t/a-beginners-attempt-at-otto-with-a-focus-on-polars/102803/13?u=daniel)


<mark style="background: #FFB8EBA6;">Datasets Safe and Easy to Use</mark> 

- otto-train-set-test-set-optimized (both seconds and milliseconds, generated purely on Kaggle): [otto-radek-style-polars](https://www.kaggle.com/datasets/danielliao/otto-radek-style-polars)
- otto-validation-split-7-days (generated purely on Kaggle): [validation-4th-optimized-parquet](https://www.kaggle.com/datasets/danielliao/validation-4th-optimized-parquet)
---
---

## <mark style="background: #FFB86CA6;">My codes</mark> 


```python
!pip install polars

import polars as pl
import pandas as pd
import random
from polars.testing import assert_frame_equal, assert_series_equal
from datetime import datetime

from IPython.core.interactiveshell import InteractiveShell
InteractiveShell.ast_node_interactivity = "all"
pd.set_option('display.max_colwidth', None)
cfg = pl.Config.restore_defaults()  
pl.Config.set_tbl_rows(50)  
pl.Config.set_fmt_str_lengths(1000)
```

^0769ee


```python
# Radek's validation set is using the older version of organizer's script (there is a little cold start problem on aid)
train_v = pl.scan_parquet('/kaggle/input/otto-train-and-test-data-for-local-validation/train.parquet')
test_v = pl.scan_parquet('/kaggle/input/otto-train-and-test-data-for-local-validation/test.parquet')

# I created this validation set on paperspace and optimized on Kaggle
train_v7 = pl.scan_parquet('/kaggle/input/ottovalidation7days/train_sessions.parquet')
test_v7 = pl.scan_parquet('/kaggle/input/ottovalidation7days/test_sessions.parquet')
test_v7_full = pl.scan_parquet('/kaggle/input/ottovalidation7days/test_sessions_full.parquet')

# I created this validation set on paperspace and optimized on Kaggle too
train_v7_2nd = pl.scan_parquet('/kaggle/input/ottovalidation7days2nd/train_sessions.parquet')
test_v7_2nd = pl.scan_parquet('/kaggle/input/ottovalidation7days2nd/test_sessions.parquet')
test_v7_full_2nd = pl.scan_parquet('/kaggle/input/ottovalidation7days2nd/test_sessions_full.parquet')

# I created this validation set on Kaggle and optimized on Kaggle too
train_v7_3rd = pl.scan_parquet('/kaggle/input/otto-validation-optimized-parquet/train_sessions.parquet')
test_v7_3rd = pl.scan_parquet('/kaggle/input/otto-validation-optimized-parquet/test_sessions.parquet')
test_v7_full_3rd = pl.scan_parquet('/kaggle/input/otto-validation-optimized-parquet/test_sessions_full.parquet')


train_ms = pl.scan_parquet('/kaggle/input/otto-radek-style-polars/train_ms.parquet')
test_ms = pl.scan_parquet('/kaggle/input/otto-radek-style-polars/test_ms.parquet')
```

^0f6921



```python
train_ms = pl.scan_parquet('/kaggle/input/otto-radek-style-polars/train_ms.parquet')
test_ms = pl.scan_parquet('/kaggle/input/otto-radek-style-polars/test_ms.parquet')

split_ts_ms = train_ms.select([
    (pl.col('ts').max() - 7*24*60*60*1000).alias('split_ts')
]).collect().to_series().to_list()[0]

train_sessions = (
    train_ms
    .filter(~(pl.col('ts').first() > split_ts_ms).over('session'))
    .filter(pl.col('ts') < split_ts_ms)
    .filter((pl.col('aid').count()>=2).over('session'))
    .collect()
)    
```

^b8c496

```python
train_ms = pl.scan_parquet('/kaggle/input/otto-radek-style-polars/train_ms.parquet')
test_ms = pl.scan_parquet('/kaggle/input/otto-radek-style-polars/test_ms.parquet')

split_ts_ms = train_ms.select([
    (pl.col('ts').max() - 7*24*60*60*1000).alias('split_ts')
]).collect().to_series().to_list()[0]

# building `train_items` as it is in organizer's script https://github.com/otto-de/recsys-dataset/blob/main/src/testset.py#L100
unique_aids_train_valid = (
    train_ms
    .filter(~(pl.col('ts').first() > split_ts_ms).over('session'))
    .filter(pl.col('ts') < split_ts_ms)
    .filter((pl.col('aid').count()>=2).over('session'))
    .select([
        pl.col('aid').unique().alias('unique_aids')
    ]).collect().to_series().to_list()
)
len(unique_aids_train_valid) # 1825325

test_sessions_full = (
    train_ms
    .filter((pl.col('ts').first() > split_ts_ms).over('session')).collect() # stop being lazy early can avoid error and wrong answers
    .filter(pl.col('aid').is_in(unique_aids_train_valid)) # step 2-2: https://github.com/otto-de/recsys-dataset/blob/main/src/testset.py#L75
    .filter((pl.col('aid').count()>=2).over('session')) # step 3: https://github.com/otto-de/recsys-dataset/blob/main/src/testset.py#L76

)
```

^2676d6

```python
test_v7_full_3rd = pl.scan_parquet('/kaggle/input/otto-validation-optimized-parquet/test_sessions_full.parquet')
test_sessions_full = test_v7_full_3rd.collect()

test_sessions_full = (
    test_sessions_full
    .select([
        pl.all().shift(1).over('session') # remove the last event of each session, see code in step 5 of ground_truth
    ])
    .drop_nulls()
)


sess_length = (
    test_sessions_full
    .groupby('session').agg([
        pl.col('aid').count().alias('sess_length'),
    ])
    .sort('session')
    .select('sess_length')
    .to_series().to_list()
)

random.seed(42)
split_idx_sess = [random.randint(1, x) for x in sess_length] # random seed will work here

test_sessions_split = (
    test_sessions_full
    .groupby('session').agg([
        pl.col('aid').count().alias('sess_length'),
    ])
    .sort('session')
    .with_columns([
        pl.Series(name='split_idx', values=split_idx_sess)
    ])
)

test_sessions_handmade = (
    test_sessions_full
    .with_columns([
        pl.col('aid').cumcount().over('session').alias('event_idx_each_session'),
        pl.col('aid').count().over('session').alias('session_length'),
    ])
    .join(test_sessions_split, on='session')
    .filter(pl.col('event_idx_each_session') < pl.col('split_idx'))
    .select([
        'session',
        'aid',
        'ts',
        'type'
    ])
)
```

^f55509


```python
test_sessions_full = test_v7_full_3rd

test_labels_sessions = (
    test_sessions_full
    .collect()
    .with_columns([
        pl.col('aid').cumcount().over('session').alias('event_idx_each_session'),
        pl.col('aid').count().over('session').alias('session_length'),
    ])
    .join(test_sessions_split, on='session') # include the column 'split_idx' which take into account that the last event removed
    .filter(~(pl.col('event_idx_each_session') < pl.col('split_idx'))) # the last event of each session is preserved here.
)

test_labels = (
    test_labels_sessions
    .with_columns([
        pl.col('aid').filter(pl.col('type') == 0).first().over('session').alias('label_clicks'),# won't turned into a list
        pl.col('aid').filter(pl.col('type') == 1).unique().list().over('session').alias('label_carts'),
        pl.col('aid').filter(pl.col('type') == 2).unique().list().over('session').alias('label_orders'),        
    ])
    .with_columns([
        pl.concat_list(['label_clicks']),# make it a list
        pl.col('label_orders').arr.eval(pl.element().cast(pl.Utf8)).alias("str"), 
    ])
    .select([
        pl.all().exclude(['str', 'event_idx_each_session', 'session_length', 'sess_length', 'split_idx']),
        pl.col('str').arr.join(" ").alias('label_orders_str')
    ])
    .groupby('session').agg([
        pl.all().first()
    ])
    .sort('session')
)


label_clicks = test_labels.select([
    pl.col('session'),
    pl.lit('clicks').alias('type'),
    pl.col('label_clicks').alias('ground_truth'),
]).filter(pl.col('ground_truth').arr.first().is_not_null())


label_carts = test_labels.select([
    pl.col('session'),
    pl.lit('carts').alias('type'),
    pl.col('label_carts').alias('ground_truth'),
]).filter(pl.col('ground_truth').arr.lengths()>0)


label_orders = test_labels.select([
    pl.col('session'),
    pl.lit('orders').alias('type'),
    pl.col('label_orders').alias('ground_truth'),
]).filter(pl.col('ground_truth').arr.lengths()>0)


test_labels_handmade = pl.concat([label_clicks, label_carts, label_orders]).sort('session').select([
    pl.col('session').cast(pl.Int32),
    pl.col('type'),
    pl.col('ground_truth').arr.eval(pl.element().cast(pl.Int32)) # cast into Int32 for each element of a list in a column
])    
```

^eb3235

```python
from datetime import datetime

(
    train_ms
    .select([
        pl.lit(datetime(2022, 7, 31)).alias('2022-7-31'),
        pl.lit(datetime(2022, 8, 1)).alias('2022-8-1'),        
    ])
    .collect()
)
```

^5ac544

```python
from datetime import datetime

(
    train_ms
    .filter(pl.col('ts').cast(pl.Datetime(time_unit='ms')).is_between(datetime(2022, 8, 1), datetime(2022, 8, 2)))    
    .with_columns([
        pl.col('ts').cast(pl.Datetime).dt.with_time_unit('ms')
    ])
    .select([
        pl.col('ts').min().alias("min_datetime"), 
        pl.col('ts').max().alias("max_datetime"), 
        pl.col('ts').first().alias("first_datetime"),     
        pl.col('ts').last().alias("last_datetime"), 
    ])
    .collect()
)
```

^9a15dc

```python
train_ms = pl.scan_parquet('/kaggle/input/otto-radek-style-polars/train_ms.parquet')
test_ms = pl.scan_parquet('/kaggle/input/otto-radek-style-polars/test_ms.parquet')
train_sessions = pl.scan_parquet('/kaggle/input/validation-4th-optimized-parquet/train_sessions.parquet')
test_sessions_full = pl.scan_parquet('/kaggle/input/validation-4th-optimized-parquet/train_sessions.parquet')

(
    train_ms
    .select([
        pl.col('session').n_unique().alias('total_sessions'),
        pl.col('aid').n_unique().alias('total_aid'),    
        pl.col('session').count().alias('total_rows'),      
        pl.col('ts').min().cast(pl.Datetime).dt.with_time_unit('ms').alias("min_datetime"), 
        pl.col('ts').max().cast(pl.Datetime).dt.with_time_unit('ms').alias("max_datetime"), 
        pl.col('ts').first().cast(pl.Datetime).dt.with_time_unit('ms').alias("first_datetime"),     
        pl.col('ts').last().cast(pl.Datetime).dt.with_time_unit('ms').alias("last_datetime"), 
    ])
    .collect()
)

```

^3a334e

```python
train_ms = pl.scan_parquet('/kaggle/input/otto-radek-style-polars/train_ms.parquet')

(
    train_ms
    .select([
        pl.duration(microseconds=(pl.col('ts').last() - pl.col('ts').first())).alias('if_unit_us'),                
        pl.duration(milliseconds=(pl.col('ts').last() - pl.col('ts').first())).alias('if_unit_ms'),
        pl.duration(seconds=(pl.col('ts').last() - pl.col('ts').first())).alias('if_unit_sec'),        
    ])
     .collect()
 )
```

^69336f

```python
(
    train_ms
    .filter(pl.col('session') == 12017380)
    .select([
        pl.duration(milliseconds=(pl.col('ts').last() - pl.col('ts').first())).alias('sess_duration')
    ])
    .collect()
    
)


(
    train_ms
    .groupby('session').agg([
        (pl.col('ts').last() - pl.col('ts').first()).cast(pl.Duration(time_unit='us')).alias('duration_us'),
        (pl.col('ts').last() - pl.col('ts').first()).cast(pl.Duration(time_unit='ms')).alias('duration_ms'),        
        (pl.col('ts').last() - pl.col('ts').first()).cast(pl.Datetime(time_unit='ms')).dt.hour().alias('hr_from_dur'),
        (pl.col('ts').last() - pl.col('ts').first()).cast(pl.Duration(time_unit='ms')).dt.hours().alias('dur_in_hrs'),        
        ((pl.col('ts').last() - pl.col('ts').first()).cast(pl.Duration(time_unit='ms'))/(1000*60*60)).alias('dur_in_hrs_hand'),
        (pl.col('ts').last() - pl.col('ts').first()).cast(pl.Duration(time_unit='ms')).dt.days().alias('dur_in_days'),                
        ((pl.col('ts').last() - pl.col('ts').first()).cast(pl.Duration(time_unit='ms'))/(1000*60*60*24)).alias('dur_in_days_hand'),        
        
    ])
     .collect()
 )

```

^d51937

```python
train_ms = pl.scan_parquet('/kaggle/input/otto-radek-style-polars/train_ms.parquet')
(
    train_ms
    .select([
        pl.col('ts').min().cast(pl.Datetime(time_unit='ms')).alias("min_datetime"), 
        pl.col('ts').max().cast(pl.Datetime(time_unit='ms')).alias("max_datetime"), 
        pl.col('ts').first().cast(pl.Datetime(time_unit='ms')).alias("first_datetime"),     
        pl.col('ts').last().cast(pl.Datetime(time_unit='ms')).alias("last_datetime"), 
    ])
    .collect()
)
```

^00fed4


```python
train_ms = pl.scan_parquet('/kaggle/input/otto-radek-style-polars/train_ms.parquet')
(
    train_ms
    .with_columns([
        pl.col('ts').cast(pl.Datetime(time_unit='ms')).alias("ts"), 
        pl.col('ts').cast(pl.Datetime(time_unit='ms')).dt.year().alias("year"),         
        pl.col('ts').cast(pl.Datetime(time_unit='ms')).dt.month().alias("month"),         
        pl.col('ts').cast(pl.Datetime(time_unit='ms')).dt.day().alias("day"),  
        pl.col('ts').cast(pl.Datetime(time_unit='ms')).dt.ordinal_day().alias("ordinal_day"),         
        pl.col('ts').cast(pl.Datetime(time_unit='ms')).dt.hour().alias("hour"),             
        pl.col('ts').cast(pl.Datetime(time_unit='ms')).dt.minute().alias("min"),   
        pl.col('ts').cast(pl.Datetime(time_unit='ms')).dt.second().alias("sec"),           
#         (pl.col('ts') - pl.col('ts').first()).cast(pl.Duration(time_unit='ms')).dt.hours().over('session').alias("second"),         
    ])
    .fetch()
    
)
```

^7f5465

```python
duration_since_sessions = (
    train_ms
    .with_columns([
        pl.col('ts').cast(pl.Datetime(time_unit='ms')).alias("ts"),         
        (pl.col('ts') - pl.col('ts').first()).cast(pl.Duration(time_unit='ms')).alias('duration'),        
        (pl.col('ts') - pl.col('ts').first()).cast(pl.Duration(time_unit='ms')).dt.days().over('session').alias("days_since_start_each_sess"),        
        (pl.col('ts') - pl.col('ts').first()).cast(pl.Duration(time_unit='ms')).dt.hours().over('session').alias("hrs_since_start_each_sess"),
        (pl.col('ts') - pl.col('ts').first()).cast(pl.Duration(time_unit='ms')).dt.minutes().over('session').alias("mins_since_start_each_sess"),
        (pl.col('ts') - pl.col('ts').first()).cast(pl.Duration(time_unit='ms')).dt.seconds().over('session').alias("secs_since_start_each_sess"),        
    ])
    .collect()
)
duration_since_sessions
```

^4aec22

```python
(
    duration_since_sessions
    .groupby('session')
    .head()
    .sort('session')
)
```

^fa70d5

```python
# When the training set start and end and how the data is extracted according to time
(
    train_ms
    .with_columns([
        pl.col('ts').cast(pl.Datetime(time_unit='ms'))
    ])
    .select([
        pl.col('ts').first().alias('start_time_training_set'),
        pl.col('ts').last().alias('end_time_training_set'),        
        pl.col('ts').min().alias('min_time_training_set'),
        pl.col('ts').max().alias('max_time_training_set'),                
    ])
    .collect()
)
```

^8fbdee

```python
# How to select 7 days from the first day's 22:00 to the last day's 22:00
# the train_ms starts from 2022-07-31 22:00:00.025 and ends at 2022-08-28 21:59:59.984, so my first 7 days subset should follow this time constraint
# this following code can subset and verify the data

(
    train_ms
    .filter(pl.col('ts').cast(pl.Datetime(time_unit='ms')).is_between(datetime(2022,7,31,22,0,0), datetime(2022,8,7,22,0,0)))
    .with_columns([
        pl.col('ts').cast(pl.Datetime(time_unit='ms')), # save some typing of `cast...`
    ])
    .with_columns([
        pl.col('ts').first().over('session').alias('start_datetime_each_session'),
        pl.col('ts').last().over('session').alias('end_datetime_each_session'),     
    ])
    .groupby('session')
    .head(2)
    .sort('start_datetime_each_session')
    # .sort('end_datetime_each_session', reverse=True)
    .collect()
)
```

^a94741

```python
from IPython.core.interactiveshell import InteractiveShell
InteractiveShell.ast_node_interactivity = "all"

from sympy.plotting import plot 
from sympy import Symbol

sess_len = 10
action_num_reverse = Symbol('x')

linear_interpolation = 0.1 + ((1-0.1)/(sess_len-1))*(sess_len-action_num_reverse-1)
log_recency_score = 2**line1 - 1
p = plot(linear_interpolation, log_recency_score, (action_num_reverse, 9, 0), legend=True, show=False)
p[0].line_color = 'b'
p[1].line_color = 'r'
p.show()
```

^ec82df

```python
train_sessions_full = pl.scan_parquet('/kaggle/input/1st-7days-train-validation-set/train_sessions_full.parquet')
train_sessions = pl.scan_parquet('/kaggle/input/1st-7days-train-validation-set/train_sessions.parquet')
test_sessions_full = pl.scan_parquet('/kaggle/input/1st-7days-train-validation-set/test_sessions_full.parquet')
test_sessions = pl.scan_parquet('/kaggle/input/1st-7days-train-validation-set/test_sessions.parquet')
test_labels = pl.scan_parquet('/kaggle/input/1st-7days-train-validation-set/test_labels.parquet')

# There should be no aids of `test_sessions` unknown to `train_sessions`
(
    test_sessions
    .select([
        pl.col('aid').is_in(train_sessions.select('aid').unique().collect().to_series().to_list()).alias('known_aid_to_train?')
    ])
    .select([
        (~pl.col('known_aid_to_train?')).alias('new_aid_to_train?')
    ])
    .select([
        pl.col('new_aid_to_train?').sum().alias('num_aids_unknown_to_train')
    ])
    .collect()
)

# There should be no overlap sessions between `train_sessions` and `test_sessions`
(
    test_sessions
    .select([
        pl.col('session').unique().is_in(train_sessions.select('session').unique().collect().to_series().to_list()).alias('known_sess_to_train?')
    ])
    .select([
        pl.col('known_sess_to_train?').sum().alias('num_sess_known_to_train')
    ])
    .collect()
)
```